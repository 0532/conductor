{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Conductor \u00b6 Conductor is an orchestration engine that runs in the cloud. Motivation \u00b6 We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features: Allow creating complex process / business flows in which individual task is implemented by a microservice. A JSON DSL based blueprint defines the execution flow. Provide visibility and traceability into the these process flows. Expose control semantics around pause, resume, restart, etc allowing for better devops experience. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize the process flows. Ability to synchronously process all the tasks when needed. Ability to scale millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Why not peer to peer choreography? With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201chow much are we done with process X\u201d?","title":"Introduction"},{"location":"#conductor","text":"Conductor is an orchestration engine that runs in the cloud.","title":"Conductor"},{"location":"#motivation","text":"We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features: Allow creating complex process / business flows in which individual task is implemented by a microservice. A JSON DSL based blueprint defines the execution flow. Provide visibility and traceability into the these process flows. Expose control semantics around pause, resume, restart, etc allowing for better devops experience. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize the process flows. Ability to synchronously process all the tasks when needed. Ability to scale millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Why not peer to peer choreography? With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201chow much are we done with process X\u201d?","title":"Motivation"},{"location":"faq/","text":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.) \u00b6 After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT. How long can a workflow be in running state? Can I have a workflow that keeps running for days or months? \u00b6 Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state. My workflow fails to start with missing task error \u00b6 Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again. Where does my worker run? How does conductor run my tasks? \u00b6 Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server. How can I schedule workflows to run at a specific time? \u00b6 Conductor does not provide any scheduling mechanism. But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing . How do I setup Dynomite cluster? \u00b6 Visit Dynomite's github page. https://github.com/Netflix/dynomite to find details on setup and support mechanism. Can I use conductor with Ruby / Go / Python? \u00b6 Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python client is currently in development and not battle tested for production use cases. How can I get help with Dynomite? \u00b6 Visit Dynomite's github page. https://github.com/Netflix/dynomite to find details on setup and support mechanism.","title":"FAQ"},{"location":"faq/#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","text":"After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT.","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)"},{"location":"faq/#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","text":"Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?"},{"location":"faq/#my-workflow-fails-to-start-with-missing-task-error","text":"Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again.","title":"My workflow fails to start with missing task error"},{"location":"faq/#where-does-my-worker-run-how-does-conductor-run-my-tasks","text":"Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server.","title":"Where does my worker run?  How does conductor run my tasks?"},{"location":"faq/#how-can-i-schedule-workflows-to-run-at-a-specific-time","text":"Conductor does not provide any scheduling mechanism. But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing .","title":"How can I schedule workflows to run at a specific time?"},{"location":"faq/#how-do-i-setup-dynomite-cluster","text":"Visit Dynomite's github page. https://github.com/Netflix/dynomite to find details on setup and support mechanism.","title":"How do I setup Dynomite cluster?"},{"location":"faq/#can-i-use-conductor-with-ruby-go-python","text":"Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python client is currently in development and not battle tested for production use cases.","title":"Can I use conductor with Ruby / Go / Python?"},{"location":"faq/#how-can-i-get-help-with-dynomite","text":"Visit Dynomite's github page. https://github.com/Netflix/dynomite to find details on setup and support mechanism.","title":"How can I get help with Dynomite?"},{"location":"license/","text":"Copyright 2018 Netflix, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"domains/","text":"Task Domains \u00b6 Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active then the task is schedule with no domain (default behavior). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. How to use Task Domains \u00b6 Change the poll call \u00b6 The poll call must now specify the domain. Java Client \u00b6 If you are using the java client then a simple property change will force WorkflowTaskCoordinator to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\" REST call \u00b6 GET /tasks/poll/batch/T2?workerid=myworker&domain=mydomain GET /tasks/poll/T2?workerid=myworker&domain=mydomain Change the start workflow call \u00b6 When starting the workflow, make sure the task to domain mapping is passes Java Client \u00b6 Map<String, Object> input = new HashMap<>(); input.put(\"wf_input1\", \"one\u201d); Map<String, String> taskToDomain = new HashMap<>(); taskToDomain.put(\"T2\", \"mydomain\"); // Other options ... // taskToDomain.put(\"*\", \"mydomain\") will put all tasks in mydomain // taskToDomain.put(\"T2\", \"mydomain,fallbackDomain\") If mydomain has no active workers // for T2 then will be put in fallbackDomain. Same can be used with \"*\" too. StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.start_workflow(swr); REST call \u00b6 POST /workflow { \"name\": \"myWorkflow\", \"version\": 1, \"correlatonId\": \"corr1\" \"input\": { \"wf_input1\": \"one\" }, \"taskToDomain\": { \"T2\": \"mydomain\" } }","title":"Task Domains"},{"location":"domains/#task-domains","text":"Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active then the task is schedule with no domain (default behavior). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used.","title":"Task Domains"},{"location":"domains/#how-to-use-task-domains","text":"","title":"How to use Task Domains"},{"location":"domains/#change-the-poll-call","text":"The poll call must now specify the domain.","title":"Change the poll call"},{"location":"domains/#java-client","text":"If you are using the java client then a simple property change will force WorkflowTaskCoordinator to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\"","title":"Java Client"},{"location":"domains/#rest-call","text":"GET /tasks/poll/batch/T2?workerid=myworker&domain=mydomain GET /tasks/poll/T2?workerid=myworker&domain=mydomain","title":"REST call"},{"location":"domains/#change-the-start-workflow-call","text":"When starting the workflow, make sure the task to domain mapping is passes","title":"Change the start workflow call"},{"location":"domains/#java-client_1","text":"Map<String, Object> input = new HashMap<>(); input.put(\"wf_input1\", \"one\u201d); Map<String, String> taskToDomain = new HashMap<>(); taskToDomain.put(\"T2\", \"mydomain\"); // Other options ... // taskToDomain.put(\"*\", \"mydomain\") will put all tasks in mydomain // taskToDomain.put(\"T2\", \"mydomain,fallbackDomain\") If mydomain has no active workers // for T2 then will be put in fallbackDomain. Same can be used with \"*\" too. StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.start_workflow(swr);","title":"Java Client"},{"location":"domains/#rest-call_1","text":"POST /workflow { \"name\": \"myWorkflow\", \"version\": 1, \"correlatonId\": \"corr1\" \"input\": { \"wf_input1\": \"one\" }, \"taskToDomain\": { \"T2\": \"mydomain\" } }","title":"REST call"},{"location":"events/","text":"Introduction \u00b6 Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations. Event Task \u00b6 Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation. Event Handler \u00b6 Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS. Configuration \u00b6 Event Handlers are configured via /event/ APIs. Structure: \u00b6 { \"name\" : \"descriptive unique name\", \"event\": \"event_type:event_location\", \"condition\": \"boolean condition\", \"actions\": [\"see examples below\"] } Condition \u00b6 Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { \"fileType\": \"AUDIO\", \"version\": 3, \"metadata\": { length: 300, codec: \"aac\" } } Expression Result $.version > 1 true $.version > 10 false $.metadata.length == 300 true Actions \u00b6 Start A Workflow { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"WORKFLOW_NAME\", \"version\": <optional> \"input\": { \"param1\": \"${param1}\" } } } Complete Task * { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${source.externalId.workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${source.result}\" } }, \"expandInlineJSON\": true } Fail Task * { \"action\": \"fail_task\", \"fail_task\": { \"workflowId\": \"${source.externalId.workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${source.result}\" } }, \"expandInlineJSON\": true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. Extending \u00b6 Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Event Handlers"},{"location":"events/#introduction","text":"Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.","title":"Introduction"},{"location":"events/#event-task","text":"Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation.","title":"Event Task"},{"location":"events/#event-handler","text":"Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS.","title":"Event Handler"},{"location":"events/#configuration","text":"Event Handlers are configured via /event/ APIs.","title":"Configuration"},{"location":"events/#structure","text":"{ \"name\" : \"descriptive unique name\", \"event\": \"event_type:event_location\", \"condition\": \"boolean condition\", \"actions\": [\"see examples below\"] }","title":"Structure:"},{"location":"events/#condition","text":"Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { \"fileType\": \"AUDIO\", \"version\": 3, \"metadata\": { length: 300, codec: \"aac\" } } Expression Result $.version > 1 true $.version > 10 false $.metadata.length == 300 true","title":"Condition"},{"location":"events/#actions","text":"Start A Workflow { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"WORKFLOW_NAME\", \"version\": <optional> \"input\": { \"param1\": \"${param1}\" } } } Complete Task * { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${source.externalId.workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${source.result}\" } }, \"expandInlineJSON\": true } Fail Task * { \"action\": \"fail_task\", \"fail_task\": { \"workflowId\": \"${source.externalId.workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${source.result}\" } }, \"expandInlineJSON\": true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions.","title":"Actions"},{"location":"events/#extending","text":"Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Extending"},{"location":"extend/","text":"Backend \u00b6 Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that needs to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementation for each of these. e.g. SQS for queueing and a relational store for others. System Tasks \u00b6 To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton)","title":"Extending Conductor"},{"location":"extend/#backend","text":"Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that needs to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementation for each of these. e.g. SQS for queueing and a relational store for others.","title":"Backend"},{"location":"extend/#system-tasks","text":"To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton)","title":"System Tasks"},{"location":"intro/","text":"High Level Architecture \u00b6 The API and storage layers are pluggable and provide ability to work with different backend and queue service providers. Installing and Running \u00b6 Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation. Running In-Memory Server \u00b6 Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purpose and does not store the data on disk. All the data is lost once the server dies. Checkout the source from github \u00b6 git clone git@github.com:Netflix/conductor.git Start Local Server \u00b6 cd server ../gradlew server # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/ Start UI Server \u00b6 cd ui gulp watch Or Start all the services using docker-compose \u00b6 cd docker docker-compose up If you ran it locally, launch UI at http://localhost:3000/ or if you have ran it using docker-compose launch the UI at http://localhost:5000/ !!!Note: The server will load a sample kitchen sink workflow definition by default. See here for details. Runtime Model \u00b6 Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing. High Level Steps \u00b6 Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { \"outputData\": { \"encodeResult\":\"success\", \"location\": \"http://cdn.example.com/file/location.png\" //any task specific output }, \"status\": \"COMPLETED\" }","title":"Getting Started"},{"location":"intro/#high-level-architecture","text":"The API and storage layers are pluggable and provide ability to work with different backend and queue service providers.","title":"High Level Architecture"},{"location":"intro/#installing-and-running","text":"Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation.","title":"Installing and Running"},{"location":"intro/#running-in-memory-server","text":"Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purpose and does not store the data on disk. All the data is lost once the server dies.","title":"Running In-Memory Server"},{"location":"intro/#checkout-the-source-from-github","text":"git clone git@github.com:Netflix/conductor.git","title":"Checkout the source from github"},{"location":"intro/#start-local-server","text":"cd server ../gradlew server # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/","title":"Start Local Server"},{"location":"intro/#start-ui-server","text":"cd ui gulp watch","title":"Start UI Server"},{"location":"intro/#or-start-all-the-services-using-docker-compose","text":"cd docker docker-compose up If you ran it locally, launch UI at http://localhost:3000/ or if you have ran it using docker-compose launch the UI at http://localhost:5000/ !!!Note: The server will load a sample kitchen sink workflow definition by default. See here for details.","title":"Or Start all the services using docker-compose"},{"location":"intro/#runtime-model","text":"Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing.","title":"Runtime Model"},{"location":"intro/#high-level-steps","text":"Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { \"outputData\": { \"encodeResult\":\"success\", \"location\": \"http://cdn.example.com/file/location.png\" //any task specific output }, \"status\": \"COMPLETED\" }","title":"High Level Steps"},{"location":"intro/concepts/","text":"Workflow Definition \u00b6 Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. more details Task Definition \u00b6 All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Worker tasks fall into two categories: System Task Worker Task System Tasks \u00b6 System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. Name Purpose DYNAMIC A worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint DECIDE Decision tasks - implements case...switch style fork FORK Forks a parallel set of tasks. Each set is scheduled to be executed in parallel FORK_JOIN_DYNAMIC Similar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task JOIN Complements FORK and FORK_JOIN_DYNAMIC. Used to merge one of more parallel branches* SUB_WORKFLOW Nest another workflow as a sub workflow task. Upon execution it instantiates the sub workflow and awaits it completion EVENT Produces an event in a supported eventing system (e.g. Conductor, SQS) Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. see WorkflowSystemTask interface for details. Worker Tasks \u00b6 Worker tasks are implemented by application(s) and runs in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST API endpoints to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint. Lifecycle of a Workflow Task \u00b6 more details","title":"Basic Concepts"},{"location":"intro/concepts/#workflow-definition","text":"Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. more details","title":"Workflow Definition"},{"location":"intro/concepts/#task-definition","text":"All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Worker tasks fall into two categories: System Task Worker Task","title":"Task Definition"},{"location":"intro/concepts/#system-tasks","text":"System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. Name Purpose DYNAMIC A worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint DECIDE Decision tasks - implements case...switch style fork FORK Forks a parallel set of tasks. Each set is scheduled to be executed in parallel FORK_JOIN_DYNAMIC Similar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task JOIN Complements FORK and FORK_JOIN_DYNAMIC. Used to merge one of more parallel branches* SUB_WORKFLOW Nest another workflow as a sub workflow task. Upon execution it instantiates the sub workflow and awaits it completion EVENT Produces an event in a supported eventing system (e.g. Conductor, SQS) Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. see WorkflowSystemTask interface for details.","title":"System Tasks"},{"location":"intro/concepts/#worker-tasks","text":"Worker tasks are implemented by application(s) and runs in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST API endpoints to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint.","title":"Worker Tasks"},{"location":"intro/concepts/#lifecycle-of-a-workflow-task","text":"more details","title":"Lifecycle of a Workflow Task"},{"location":"metadata/","text":"Task Definition \u00b6 Conductor maintains a registry of worker task types. A task type MUST be registered before using in a workflow. Example { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600 } field description Notes name Task Type Unique retryCount No. of retries to attempt when a task is marked as failure retryLogic Mechanism for the retries see possible values below timeoutSeconds Time in milliseconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 timeoutPolicy Task's timeout policy see possible values below responseTimeoutSeconds If greater than 0, the task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. outputKeys Set of keys of task's output. Used for documenting task's output Retry Logic FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : reschedule after retryDelaySeconds * attempNo Timeout Policy RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout) Workflow Definition \u00b6 Workflows are defined using a JSON based DSL. Example { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"SIMPLE\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\" } }, { \"name\": \"deploy\", \"taskReferenceName\": \"d1\", \"type\": \"SIMPLE\", \"inputParameters\": { \"fileLocation\": \"${encode.output.encodeLocation}\" } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"schemaVersion\": 2 } field description Notes name Name of the workflow description Descriptive name of the workflow version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions as described below. outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task inputParameters List of input parameters. Used for documenting the required inputs to workflow optional Tasks within Workflow \u00b6 tasks property in a workflow defines an array of tasks to be executed in that order. Below are the mandatory minimum parameters required for each task: field description Notes name Name of the task. MUST be registered as a task type with Conductor before starting workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See \"wiring inputs and outputs\" for details In addition to these parameters, additional parameters specific to the task type are required as documented here Wiring Inputs and Outputs \u00b6 Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} - - SOURCE can be either \"workflow\" or reference name of any of the task input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\", \"url\": \"${workflow.input.fileLocation}\", \"lang\": \"${loc_task.output.languages[0]}\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/${loc_task.output.fileId}/encode\", \"body\": { \"recipe\": \"${workflow.input.recipe}\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } } } Consider the following as the workflow input { \"movieId\": \"movie_123\", \"fileLocation\":\"s3://moviebucket/file123\", \"recipe\":\"png\" } And the output of the loc_task as the following; { \"fileId\": \"file_xxx_yyy_zzz\", \"languages\": [\"en\",\"ja\",\"es\"] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { \"movieId\": \"movie_123\", \"url\": \"s3://moviebucket/file123\", \"lang\": \"en\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\", \"body\": { \"recipe\": \"png\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } }","title":"Metadata Definitions"},{"location":"metadata/#task-definition","text":"Conductor maintains a registry of worker task types. A task type MUST be registered before using in a workflow. Example { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600 } field description Notes name Task Type Unique retryCount No. of retries to attempt when a task is marked as failure retryLogic Mechanism for the retries see possible values below timeoutSeconds Time in milliseconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 timeoutPolicy Task's timeout policy see possible values below responseTimeoutSeconds If greater than 0, the task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. outputKeys Set of keys of task's output. Used for documenting task's output Retry Logic FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : reschedule after retryDelaySeconds * attempNo Timeout Policy RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout)","title":"Task Definition"},{"location":"metadata/#workflow-definition","text":"Workflows are defined using a JSON based DSL. Example { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"SIMPLE\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\" } }, { \"name\": \"deploy\", \"taskReferenceName\": \"d1\", \"type\": \"SIMPLE\", \"inputParameters\": { \"fileLocation\": \"${encode.output.encodeLocation}\" } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"schemaVersion\": 2 } field description Notes name Name of the workflow description Descriptive name of the workflow version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions as described below. outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task inputParameters List of input parameters. Used for documenting the required inputs to workflow optional","title":"Workflow Definition"},{"location":"metadata/#tasks-within-workflow","text":"tasks property in a workflow defines an array of tasks to be executed in that order. Below are the mandatory minimum parameters required for each task: field description Notes name Name of the task. MUST be registered as a task type with Conductor before starting workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See \"wiring inputs and outputs\" for details In addition to these parameters, additional parameters specific to the task type are required as documented here","title":"Tasks within Workflow"},{"location":"metadata/#wiring-inputs-and-outputs","text":"Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} - - SOURCE can be either \"workflow\" or reference name of any of the task input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\", \"url\": \"${workflow.input.fileLocation}\", \"lang\": \"${loc_task.output.languages[0]}\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/${loc_task.output.fileId}/encode\", \"body\": { \"recipe\": \"${workflow.input.recipe}\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } } } Consider the following as the workflow input { \"movieId\": \"movie_123\", \"fileLocation\":\"s3://moviebucket/file123\", \"recipe\":\"png\" } And the output of the loc_task as the following; { \"fileId\": \"file_xxx_yyy_zzz\", \"languages\": [\"en\",\"ja\",\"es\"] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { \"movieId\": \"movie_123\", \"url\": \"s3://moviebucket/file123\", \"lang\": \"en\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\", \"body\": { \"recipe\": \"png\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } }","title":"Wiring Inputs and Outputs"},{"location":"metadata/kitchensink/","text":"An example kitchensink workflow that demonstrates the usage of all the schema constructs. Definition \u00b6 { \"name\": \"kitchensink\", \"description\": \"kitchensink workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"event_task\", \"taskReferenceName\": \"event_0\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"dyntask\", \"taskReferenceName\": \"task_2\", \"inputParameters\": { \"taskToExecute\": \"${workflow.input.task2Name}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" }, { \"name\": \"oddEvenDecision\", \"taskReferenceName\": \"oddEvenDecision\", \"inputParameters\": { \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"oddEven\", \"decisionCases\": { \"0\": [ { \"name\": \"task_4\", \"taskReferenceName\": \"task_4\", \"inputParameters\": { \"mod\": \"${task_2.output.mod}\", \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"dynamic_fanout\", \"taskReferenceName\": \"fanout1\", \"inputParameters\": { \"dynamicTasks\": \"${task_4.output.dynamicTasks}\", \"input\": \"${task_4.output.inputs}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"input\" }, { \"name\": \"dynamic_join\", \"taskReferenceName\": \"join1\", \"type\": \"JOIN\" } ], \"1\": [ { \"name\": \"fork_join\", \"taskReferenceName\": \"forkx\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"task_10\", \"taskReferenceName\": \"task_10\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf3\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ], [ { \"name\": \"task_11\", \"taskReferenceName\": \"task_11\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf4\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ] ] }, { \"name\": \"join\", \"taskReferenceName\": \"join2\", \"type\": \"JOIN\", \"joinOn\": [ \"wf3\", \"wf4\" ] } ] } }, { \"name\": \"search_elasticsearch\", \"taskReferenceName\": \"get_es_1\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } }, \"type\": \"HTTP\" }, { \"name\": \"task_30\", \"taskReferenceName\": \"task_30\", \"inputParameters\": { \"statuses\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"statues\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"schemaVersion\": 2 } Visual Flow \u00b6 Running Kitchensink Workflow \u00b6 Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definition and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status. Start workflow execution \u00b6 Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { \"task2Name\": \"task_5\" } ' The response is a text string identifying the workflow instance id. Poll for the first task: \u00b6 curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { \"taskType\": \"task_1\", \"status\": \"IN_PROGRESS\", \"inputData\": { \"mod\": null, \"oddEven\": null }, \"referenceTaskName\": \"task_1\", \"retryCount\": 0, \"seq\": 1, \"pollCount\": 1, \"taskDefName\": \"task_1\", \"scheduledTime\": 1486580932471, \"startTime\": 1486580933869, \"endTime\": 0, \"updateTime\": 1486580933902, \"startDelayInSeconds\": 0, \"retried\": false, \"callbackFromWorker\": true, \"responseTimeoutSeconds\": 3600, \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"callbackAfterSeconds\": 0, \"polledTime\": 1486580933902, \"queueWaitTime\": 1398 } Update the task status \u00b6 Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"status\": \"COMPLETED\", \"output\": { \"mod\": 5, \"taskToExecute\": \"task_1\", \"oddEven\": 0, \"dynamicTasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1_1\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_4\", \"taskReferenceName\": \"wf_dyn\", \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\" } } ], \"inputs\": { \"task_1_1\": {}, \"wf_dyn\": {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion. Using Client Libraries Conductor provides client libraries in Java (a Python client is works) to simplify task polling and execution.","title":"Kitchensink Example"},{"location":"metadata/kitchensink/#definition","text":"{ \"name\": \"kitchensink\", \"description\": \"kitchensink workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"event_task\", \"taskReferenceName\": \"event_0\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"dyntask\", \"taskReferenceName\": \"task_2\", \"inputParameters\": { \"taskToExecute\": \"${workflow.input.task2Name}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" }, { \"name\": \"oddEvenDecision\", \"taskReferenceName\": \"oddEvenDecision\", \"inputParameters\": { \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"oddEven\", \"decisionCases\": { \"0\": [ { \"name\": \"task_4\", \"taskReferenceName\": \"task_4\", \"inputParameters\": { \"mod\": \"${task_2.output.mod}\", \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"dynamic_fanout\", \"taskReferenceName\": \"fanout1\", \"inputParameters\": { \"dynamicTasks\": \"${task_4.output.dynamicTasks}\", \"input\": \"${task_4.output.inputs}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"input\" }, { \"name\": \"dynamic_join\", \"taskReferenceName\": \"join1\", \"type\": \"JOIN\" } ], \"1\": [ { \"name\": \"fork_join\", \"taskReferenceName\": \"forkx\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"task_10\", \"taskReferenceName\": \"task_10\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf3\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ], [ { \"name\": \"task_11\", \"taskReferenceName\": \"task_11\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf4\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ] ] }, { \"name\": \"join\", \"taskReferenceName\": \"join2\", \"type\": \"JOIN\", \"joinOn\": [ \"wf3\", \"wf4\" ] } ] } }, { \"name\": \"search_elasticsearch\", \"taskReferenceName\": \"get_es_1\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } }, \"type\": \"HTTP\" }, { \"name\": \"task_30\", \"taskReferenceName\": \"task_30\", \"inputParameters\": { \"statuses\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"statues\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"schemaVersion\": 2 }","title":"Definition"},{"location":"metadata/kitchensink/#visual-flow","text":"","title":"Visual Flow"},{"location":"metadata/kitchensink/#running-kitchensink-workflow","text":"Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definition and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status.","title":"Running Kitchensink Workflow"},{"location":"metadata/kitchensink/#start-workflow-execution","text":"Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { \"task2Name\": \"task_5\" } ' The response is a text string identifying the workflow instance id.","title":"Start workflow execution"},{"location":"metadata/kitchensink/#poll-for-the-first-task","text":"curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { \"taskType\": \"task_1\", \"status\": \"IN_PROGRESS\", \"inputData\": { \"mod\": null, \"oddEven\": null }, \"referenceTaskName\": \"task_1\", \"retryCount\": 0, \"seq\": 1, \"pollCount\": 1, \"taskDefName\": \"task_1\", \"scheduledTime\": 1486580932471, \"startTime\": 1486580933869, \"endTime\": 0, \"updateTime\": 1486580933902, \"startDelayInSeconds\": 0, \"retried\": false, \"callbackFromWorker\": true, \"responseTimeoutSeconds\": 3600, \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"callbackAfterSeconds\": 0, \"polledTime\": 1486580933902, \"queueWaitTime\": 1398 }","title":"Poll for the first task:"},{"location":"metadata/kitchensink/#update-the-task-status","text":"Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"status\": \"COMPLETED\", \"output\": { \"mod\": 5, \"taskToExecute\": \"task_1\", \"oddEven\": 0, \"dynamicTasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1_1\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_4\", \"taskReferenceName\": \"wf_dyn\", \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\" } } ], \"inputs\": { \"task_1_1\": {}, \"wf_dyn\": {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion. Using Client Libraries Conductor provides client libraries in Java (a Python client is works) to simplify task polling and execution.","title":"Update the task status"},{"location":"metadata/systask/","text":"Dynamic Task \u00b6 Parameters: \u00b6 name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example \u00b6 { \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task. Decision \u00b6 A decision task is similar to case...switch statement in a programming language. The task takes 3 parameters: Parameters: \u00b6 name description caseValueParam Name of the parameter in task input whose value will be used as a switch. decisionCases Map where key is possible values of caseValueParam with value being list of tasks to be executed. defaultCase List of tasks to be executed when no matching value if found in decision case (default condition) Example \u00b6 { \"name\": \"decide_task\", \"taskReferenceName\": \"decide1\", \"inputParameters\": { \"case_value_param\": \"${workflow.input.movieType}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"Show\": [ { \"name\": \"setup_episodes\", \"taskReferenceName\": \"se1\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"generate_episode_artwork\", \"taskReferenceName\": \"ga\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" } ], \"Movie\": [ { \"name\": \"setup_movie\", \"taskReferenceName\": \"sm\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"generate_movie_artwork\", \"taskReferenceName\": \"gma\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" } ] } } Fork \u00b6 Fork is used to schedule parallel set of tasks. Parameters: \u00b6 name description forkTasks A list of list of tasks. Each sublist is scheduled to be executed in parallel. However, tasks within the sublists are scheduled in a serial fashion. Example \u00b6 { \"forkTasks\": [ [ { \"name\": \"task11\", \"taskReferenceName\": \"t11\" }, { \"name\": \"task12\", \"taskReferenceName\": \"t12\" } ], [ { \"name\": \"task21\", \"taskReferenceName\": \"t21\" }, { \"name\": \"task22\", \"taskReferenceName\": \"t22\" } ] ] } When executed, task11 and task21 are scheduled to be executed at the same time. Dynamic Fork \u00b6 A dynamic fork is same as FORK_JOIN task. Except that the list of tasks to be forked is provided at runtime using task's input. Useful when number of tasks to be forked is not fixed and varies based on the input. name description dynamicForkTasksParam Name of the parameter that contains list of workflow task configuration to be executed in parallel dynamicForkTasksInputParamName Name of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task Example \u00b6 { \"inputParameters\": { \"dynamicTasks\": \"${taskA.output.dynamicTasksJSON}\", \"dynamicTasksInput\": \"${taskA.output.dynamicTasksInputJSON}\" } \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\" } Consider taskA 's output as: { \"dynamicTasksInputJSON\": { \"forkedTask1\": { \"width\": 100, \"height\": 100, \"params\": { \"recipe\": \"jpg\" } }, \"forkedTask2\": { \"width\": 200, \"height\": 200, \"params\": { \"recipe\": \"jpg\" } } }, \"dynamicTasksJSON\": [ { \"name\": \"encode_task\", \"taskReferenceName\": \"forkedTask1\", \"type\": \"SIMPLE\" }, { \"name\": \"encode_task\", \"taskReferenceName\": \"forkedTask2\", \"type\": \"SIMPLE\" } ] } When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_ Dynamic Fork and Join A Join task MUST follow FORK_JOIN_DYNAMIC Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task. However, given the dynamic nature of the task, no joinOn parameters are required for this Join. The join will wait for ALL the forked branches to complete before completing. Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows. Join \u00b6 Join task is used to wait for completion of one or more tasks spawned by fork tasks. Parameters \u00b6 name description joinOn List of task reference name, for which the JOIN will wait for completion. Example \u00b6 { \"joinOn\": [\"taskRef1\", \"taskRef3\"] } Join Task Output \u00b6 Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task. Sub Workflow \u00b6 Sub Workflow task allows for nesting a workflow within another workflow. Parameters \u00b6 name description subWorkflowParam List of task reference name, for which the JOIN will wait for completion. Example \u00b6 { \"name\": \"sub_workflow_task\", \"taskReferenceName\": \"sub1\", \"inputParameters\": { \"requestId\": \"${workflow.input.requestId}\", \"file\": \"${encode.output.location}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"deployment_workflow\", \"version\": 1 } } When executed, a deployment_workflow is executed with two input parameters requestId and file . The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. Wait \u00b6 A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT Parameters \u00b6 None required. External Triggers for Wait Task \u00b6 Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED . SQS Queues \u00b6 SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message. Example SQS Payload: \u00b6 { \"some_key\": \"valuex\", \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\" } HTTP \u00b6 An HTTP task is used to make calls to another microservice over HTTP. Parameters \u00b6 The task expects an input parameter named http_request as part of the task's input with the following details: name description uri URI for the service. Can be a partial when using vipAddress or includes the server address. method HTTP method. One of the GET, PUT, POST, DELETE, OPTIONS, HEAD accept Accept header as required by server. contentType Content Type - supported types are text/plain, text/html and, application/json headers A map of additional http headers to be sent along with the request. body Request body vipAddress When using discovery based service URLs. HTTP Task Output \u00b6 name description response JSON body containing the response if one is present headers Response Headers statusCode Integer status code Example \u00b6 Task Input payload using vipAddress { \"http_request\": { \"vipAddress\": \"examplevip-prod\", \"uri\": \"/\", \"method\": \"GET\", \"accept\": \"text/plain\" } } Task Input using an absolute URL { \"http_request\": { \"uri\": \"http://example.com/\", \"method\": \"GET\", \"accept\": \"text/plain\" } } The task is marked as FAILED if the request cannot be completed or the remote server returns non successful status code. Note HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response. XML input/output is currently not supported. However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value. Event \u00b6 Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. Parameters \u00b6 name description sink Qualified name of the event that is produced. e.g. conductor or sqs:sqs_queue_name Example \u00b6 { \"sink\": 'sqs:example_sqs_queue_name' } When producing an event with Conductor as sink, the event name follows the structure: conductor:<workflow_name>:<task_reference_name> For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs. Supported Sinks \u00b6 Conductor SQS Event Task Input \u00b6 The input given to the event task is made available to the published message as payload. e.g. if a message is put into SQS queue (sink is sqs) then the message payload will be the input to the task. Event Task Output \u00b6 event_produced Name of the event produced.","title":"System Tasks"},{"location":"metadata/systask/#dynamic-task","text":"","title":"Dynamic Task"},{"location":"metadata/systask/#parameters","text":"name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.","title":"Parameters:"},{"location":"metadata/systask/#example","text":"{ \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task.","title":"Example"},{"location":"metadata/systask/#decision","text":"A decision task is similar to case...switch statement in a programming language. The task takes 3 parameters:","title":"Decision"},{"location":"metadata/systask/#parameters_1","text":"name description caseValueParam Name of the parameter in task input whose value will be used as a switch. decisionCases Map where key is possible values of caseValueParam with value being list of tasks to be executed. defaultCase List of tasks to be executed when no matching value if found in decision case (default condition)","title":"Parameters:"},{"location":"metadata/systask/#example_1","text":"{ \"name\": \"decide_task\", \"taskReferenceName\": \"decide1\", \"inputParameters\": { \"case_value_param\": \"${workflow.input.movieType}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"Show\": [ { \"name\": \"setup_episodes\", \"taskReferenceName\": \"se1\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"generate_episode_artwork\", \"taskReferenceName\": \"ga\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" } ], \"Movie\": [ { \"name\": \"setup_movie\", \"taskReferenceName\": \"sm\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"generate_movie_artwork\", \"taskReferenceName\": \"gma\", \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\" }, \"type\": \"SIMPLE\" } ] } }","title":"Example"},{"location":"metadata/systask/#fork","text":"Fork is used to schedule parallel set of tasks.","title":"Fork"},{"location":"metadata/systask/#parameters_2","text":"name description forkTasks A list of list of tasks. Each sublist is scheduled to be executed in parallel. However, tasks within the sublists are scheduled in a serial fashion.","title":"Parameters:"},{"location":"metadata/systask/#example_2","text":"{ \"forkTasks\": [ [ { \"name\": \"task11\", \"taskReferenceName\": \"t11\" }, { \"name\": \"task12\", \"taskReferenceName\": \"t12\" } ], [ { \"name\": \"task21\", \"taskReferenceName\": \"t21\" }, { \"name\": \"task22\", \"taskReferenceName\": \"t22\" } ] ] } When executed, task11 and task21 are scheduled to be executed at the same time.","title":"Example"},{"location":"metadata/systask/#dynamic-fork","text":"A dynamic fork is same as FORK_JOIN task. Except that the list of tasks to be forked is provided at runtime using task's input. Useful when number of tasks to be forked is not fixed and varies based on the input. name description dynamicForkTasksParam Name of the parameter that contains list of workflow task configuration to be executed in parallel dynamicForkTasksInputParamName Name of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task","title":"Dynamic Fork"},{"location":"metadata/systask/#example_3","text":"{ \"inputParameters\": { \"dynamicTasks\": \"${taskA.output.dynamicTasksJSON}\", \"dynamicTasksInput\": \"${taskA.output.dynamicTasksInputJSON}\" } \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\" } Consider taskA 's output as: { \"dynamicTasksInputJSON\": { \"forkedTask1\": { \"width\": 100, \"height\": 100, \"params\": { \"recipe\": \"jpg\" } }, \"forkedTask2\": { \"width\": 200, \"height\": 200, \"params\": { \"recipe\": \"jpg\" } } }, \"dynamicTasksJSON\": [ { \"name\": \"encode_task\", \"taskReferenceName\": \"forkedTask1\", \"type\": \"SIMPLE\" }, { \"name\": \"encode_task\", \"taskReferenceName\": \"forkedTask2\", \"type\": \"SIMPLE\" } ] } When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_ Dynamic Fork and Join A Join task MUST follow FORK_JOIN_DYNAMIC Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task. However, given the dynamic nature of the task, no joinOn parameters are required for this Join. The join will wait for ALL the forked branches to complete before completing. Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows.","title":"Example"},{"location":"metadata/systask/#join","text":"Join task is used to wait for completion of one or more tasks spawned by fork tasks.","title":"Join"},{"location":"metadata/systask/#parameters_3","text":"name description joinOn List of task reference name, for which the JOIN will wait for completion.","title":"Parameters"},{"location":"metadata/systask/#example_4","text":"{ \"joinOn\": [\"taskRef1\", \"taskRef3\"] }","title":"Example"},{"location":"metadata/systask/#join-task-output","text":"Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.","title":"Join Task Output"},{"location":"metadata/systask/#sub-workflow","text":"Sub Workflow task allows for nesting a workflow within another workflow.","title":"Sub Workflow"},{"location":"metadata/systask/#parameters_4","text":"name description subWorkflowParam List of task reference name, for which the JOIN will wait for completion.","title":"Parameters"},{"location":"metadata/systask/#example_5","text":"{ \"name\": \"sub_workflow_task\", \"taskReferenceName\": \"sub1\", \"inputParameters\": { \"requestId\": \"${workflow.input.requestId}\", \"file\": \"${encode.output.location}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"deployment_workflow\", \"version\": 1 } } When executed, a deployment_workflow is executed with two input parameters requestId and file . The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.","title":"Example"},{"location":"metadata/systask/#wait","text":"A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT","title":"Wait"},{"location":"metadata/systask/#parameters_5","text":"None required.","title":"Parameters"},{"location":"metadata/systask/#external-triggers-for-wait-task","text":"Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED .","title":"External Triggers for Wait Task"},{"location":"metadata/systask/#sqs-queues","text":"SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message.","title":"SQS Queues"},{"location":"metadata/systask/#example-sqs-payload","text":"{ \"some_key\": \"valuex\", \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\" }","title":"Example SQS Payload:"},{"location":"metadata/systask/#http","text":"An HTTP task is used to make calls to another microservice over HTTP.","title":"HTTP"},{"location":"metadata/systask/#parameters_6","text":"The task expects an input parameter named http_request as part of the task's input with the following details: name description uri URI for the service. Can be a partial when using vipAddress or includes the server address. method HTTP method. One of the GET, PUT, POST, DELETE, OPTIONS, HEAD accept Accept header as required by server. contentType Content Type - supported types are text/plain, text/html and, application/json headers A map of additional http headers to be sent along with the request. body Request body vipAddress When using discovery based service URLs.","title":"Parameters"},{"location":"metadata/systask/#http-task-output","text":"name description response JSON body containing the response if one is present headers Response Headers statusCode Integer status code","title":"HTTP Task Output"},{"location":"metadata/systask/#example_6","text":"Task Input payload using vipAddress { \"http_request\": { \"vipAddress\": \"examplevip-prod\", \"uri\": \"/\", \"method\": \"GET\", \"accept\": \"text/plain\" } } Task Input using an absolute URL { \"http_request\": { \"uri\": \"http://example.com/\", \"method\": \"GET\", \"accept\": \"text/plain\" } } The task is marked as FAILED if the request cannot be completed or the remote server returns non successful status code. Note HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response. XML input/output is currently not supported. However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.","title":"Example"},{"location":"metadata/systask/#event","text":"Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.","title":"Event"},{"location":"metadata/systask/#parameters_7","text":"name description sink Qualified name of the event that is produced. e.g. conductor or sqs:sqs_queue_name","title":"Parameters"},{"location":"metadata/systask/#example_7","text":"{ \"sink\": 'sqs:example_sqs_queue_name' } When producing an event with Conductor as sink, the event name follows the structure: conductor:<workflow_name>:<task_reference_name> For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs.","title":"Example"},{"location":"metadata/systask/#supported-sinks","text":"Conductor SQS","title":"Supported Sinks"},{"location":"metadata/systask/#event-task-input","text":"The input given to the event task is made available to the published message as payload. e.g. if a message is put into SQS queue (sink is sqs) then the message payload will be the input to the task.","title":"Event Task Input"},{"location":"metadata/systask/#event-task-output","text":"event_produced Name of the event produced.","title":"Event Task Output"},{"location":"metrics/","text":"Conductor uses spectator to collect the metrics. Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_queue_depth Pending tasks queue depth taskType task_timeout Counter for timed out tasks taskType","title":"Server Metrics"},{"location":"metrics/client/","text":"Conductor uses spectator to collect the metrics. When using the Java client, the following metrics are published: Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.","title":"Worker Metrics"},{"location":"runtime/","text":"Task & Workflow Metadata \u00b6 Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional) Start A Workflow \u00b6 With Input only \u00b6 POST /workflow/{name}?version=&correlationId= { //JSON payload for workflow } Parameter Description version Optional. If not specified uses the latest version of the workflow correlationId User supplied Id that can be used to retrieve workflows Input \u00b6 JSON Payload to start the workflow. Mandatory. If workflow does not expect any input MUST pass an empty JSON like {} Output \u00b6 Id of the workflow (GUID) With Input and Task Domains \u00b6 POST /workflow { //JSON payload for Start workflow request } Start workflow request \u00b6 JSON for start workflow request { \"name\": \"myWorkflow\", // Name of the workflow \"version\": 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id \"input\": { // Input map. }, \"taskToDomain\": { // Task to domain map } } Output \u00b6 Id of the workflow (GUID) Retrieve Workflows \u00b6 Endpoint Description GET /workflow/{workflowId}?includeTasks=true|false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false&includeTasks=true|false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below. Search for Workflows \u00b6 Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start=&size=&sort=&freeText=&query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC:<fieldname> or DESC:<fieldname> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided. Output \u00b6 Search result as described below: { \"totalHits\": 0, \"results\": [ { \"workflowType\": \"string\", \"version\": 0, \"workflowId\": \"string\", \"correlationId\": \"string\", \"startTime\": \"string\", \"updateTime\": \"string\", \"endTime\": \"string\", \"status\": \"RUNNING\", \"input\": \"string\", \"output\": \"string\", \"reasonForIncompletion\": \"string\", \"executionTime\": 0, \"event\": \"string\" } ] } Manage Workflows \u00b6 Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution. Rerun \u00b6 Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { \"reRunFromWorkflowId\": \"string\", \"workflowInput\": {}, \"reRunFromTaskId\": \"string\", \"taskInput\": {} } Skip Task \u00b6 Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&taskReferenceName= { \"taskInput\": {}, \"taskOutput\": {} } Manage Tasks \u00b6 Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType=&taskType=&taskType Return the size of pending tasks for given task types Polling, Ack and Update Task \u00b6 These are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid=&domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count=&timeout=&workerid=&domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below. POST /tasks/{taskId}/ack Acknowledges the task received AFTER poll by worker. Schema for updating Task Result \u00b6 { \"workflowInstanceId\": \"Workflow Instance Id\", \"taskId\": \"ID of the task to be updated\", \"reasonForIncompletion\" : \"If failed, reason for failure\", \"callbackAfterSeconds\": 0, \"status\": \"IN_PROGRESS|FAILED|COMPLETED\", \"outputData\": { //JSON document representing Task execution output } } Acknowledging tasks after poll If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.","title":"APIs"},{"location":"runtime/#task-workflow-metadata","text":"Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional)","title":"Task &amp; Workflow Metadata"},{"location":"runtime/#start-a-workflow","text":"","title":"Start A Workflow"},{"location":"runtime/#with-input-only","text":"POST /workflow/{name}?version=&correlationId= { //JSON payload for workflow } Parameter Description version Optional. If not specified uses the latest version of the workflow correlationId User supplied Id that can be used to retrieve workflows","title":"With Input only"},{"location":"runtime/#input","text":"JSON Payload to start the workflow. Mandatory. If workflow does not expect any input MUST pass an empty JSON like {}","title":"Input"},{"location":"runtime/#output","text":"Id of the workflow (GUID)","title":"Output"},{"location":"runtime/#with-input-and-task-domains","text":"POST /workflow { //JSON payload for Start workflow request }","title":"With Input and Task Domains"},{"location":"runtime/#start-workflow-request","text":"JSON for start workflow request { \"name\": \"myWorkflow\", // Name of the workflow \"version\": 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id \"input\": { // Input map. }, \"taskToDomain\": { // Task to domain map } }","title":"Start workflow request"},{"location":"runtime/#output_1","text":"Id of the workflow (GUID)","title":"Output"},{"location":"runtime/#retrieve-workflows","text":"Endpoint Description GET /workflow/{workflowId}?includeTasks=true|false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false&includeTasks=true|false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below.","title":"Retrieve Workflows"},{"location":"runtime/#search-for-workflows","text":"Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start=&size=&sort=&freeText=&query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC:<fieldname> or DESC:<fieldname> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided.","title":"Search for Workflows"},{"location":"runtime/#output_2","text":"Search result as described below: { \"totalHits\": 0, \"results\": [ { \"workflowType\": \"string\", \"version\": 0, \"workflowId\": \"string\", \"correlationId\": \"string\", \"startTime\": \"string\", \"updateTime\": \"string\", \"endTime\": \"string\", \"status\": \"RUNNING\", \"input\": \"string\", \"output\": \"string\", \"reasonForIncompletion\": \"string\", \"executionTime\": 0, \"event\": \"string\" } ] }","title":"Output"},{"location":"runtime/#manage-workflows","text":"Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution.","title":"Manage Workflows"},{"location":"runtime/#rerun","text":"Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { \"reRunFromWorkflowId\": \"string\", \"workflowInput\": {}, \"reRunFromTaskId\": \"string\", \"taskInput\": {} }","title":"Rerun"},{"location":"runtime/#skip-task","text":"Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&taskReferenceName= { \"taskInput\": {}, \"taskOutput\": {} }","title":"Skip Task"},{"location":"runtime/#manage-tasks","text":"Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType=&taskType=&taskType Return the size of pending tasks for given task types","title":"Manage Tasks"},{"location":"runtime/#polling-ack-and-update-task","text":"These are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid=&domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count=&timeout=&workerid=&domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below. POST /tasks/{taskId}/ack Acknowledges the task received AFTER poll by worker.","title":"Polling, Ack and Update Task"},{"location":"runtime/#schema-for-updating-task-result","text":"{ \"workflowInstanceId\": \"Workflow Instance Id\", \"taskId\": \"ID of the task to be updated\", \"reasonForIncompletion\" : \"If failed, reason for failure\", \"callbackAfterSeconds\": 0, \"status\": \"IN_PROGRESS|FAILED|COMPLETED\", \"outputData\": { //JSON document representing Task execution output } } Acknowledging tasks after poll If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.","title":"Schema for updating Task Result"},{"location":"server/","text":"Installing \u00b6 Requirements \u00b6 Database : Dynomite Indexing Backend : Elasticsearch 2.x Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher There are 3 ways in which you can install Conductor: 1. Build from source \u00b6 To build from source, checkout the code from github and build server module using gradle build command. If you do not have gradle installed, you can run the command ./gradlew build from the project root. This produces conductor-server-all-VERSION.jar in the folder ./server/build/libs/ The jar can be executed using: java -jar conductor-server-VERSION-all.jar 2. Download pre-built binaries from jcenter or maven central \u00b6 Use the following coordinates: group artifact version com.netflix.conductor conductor-server-all 1.6.+ 3. Use the pre-configured Docker image \u00b6 To build the docker images for the conductor server and ui run the commands: cd docker docker-compose build After the docker images are built, run the following command to start the containers: docker-compose up This will create a docker container network that consists of the following images: conductor:server, conductor:ui, elasticsearch:2.4 , and dynomite. To view the UI, navigate to localhost:5000 , to view the Swagger docs, navigate to localhost:8080 . Configuration \u00b6 Conductor server uses a property file based configuration. The property file is passed to the Main class as a command line argument. java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path] log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console). Configuration Parameters \u00b6 # Database persistence model. Possible values are memory, redis, redis_cluster and dynomite. # If omitted, the persistence used is memory # # memory : The data is stored in memory and lost when the server dies. Useful for testing or demo # redis : non-Dynomite based redis instance # redis_cluster: AWS Elasticache Redis (cluster mode enabled).See [http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Clusters.Create.CON.RedisCluster.html] # redis_sentinel: Redis HA with Redis Sentinel. See [https://redis.io/topics/sentinel] # dynomite : Dynomite cluster. Use this for HA configuration. db=dynomite # Dynomite Cluster details. # format is host:port:rack separated by semicolon # for AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. The region in this case does not matter workflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e # Dynomite cluster name workflow.dynomite.cluster.name=dyno_cluster_name # Maximum connections to redis/dynomite workflow.dynomite.connection.maxConnsPerHost=31 # Namespace for the keys stored in Dynomite/Redis workflow.namespace.prefix=conductor # Namespace prefix for the dyno queues workflow.namespace.queue.prefix=conductor_queues # No. of threads allocated to dyno-queues (optional) queues.dynomite.threads=10 # Non-quorum port used to connect to local redis. Used by dyno-queues. # When using redis directly, set this to the same port as redis server. # For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite. queues.dynomite.nonQuorum.port=22122 # Transport address to elasticsearch # Specifying multiple node urls is not supported. specify one of the nodes' url, or a load balancer. workflow.elasticsearch.url=localhost:9300 # Name of the elasticsearch cluster workflow.elasticsearch.index.name=conductor # Additional modules (optional) conductor.additional.modules=class_extending_com.google.inject.AbstractModule High Availability Configuration \u00b6 Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs. The scalability of the server is achieved by scaling the Dynomite cluster along with dyno-queues which is used for queues. Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack). Using Standalone Redis / ElastiCache \u00b6 Conductor server can be used with a standlone Redis or ElastiCache server. To configure the server, change the config to use the following: db=redis # For AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. # The region in this case does not matter workflow.dynomite.cluster.hosts=server_address:server_port:us-east-1e workflow.dynomite.connection.maxConnsPerHost=31 queues.dynomite.nonQuorum.port=server_port","title":"Conductor Server"},{"location":"server/#installing","text":"","title":"Installing"},{"location":"server/#requirements","text":"Database : Dynomite Indexing Backend : Elasticsearch 2.x Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher There are 3 ways in which you can install Conductor:","title":"Requirements"},{"location":"server/#1-build-from-source","text":"To build from source, checkout the code from github and build server module using gradle build command. If you do not have gradle installed, you can run the command ./gradlew build from the project root. This produces conductor-server-all-VERSION.jar in the folder ./server/build/libs/ The jar can be executed using: java -jar conductor-server-VERSION-all.jar","title":"1. Build from source"},{"location":"server/#2-download-pre-built-binaries-from-jcenter-or-maven-central","text":"Use the following coordinates: group artifact version com.netflix.conductor conductor-server-all 1.6.+","title":"2. Download pre-built binaries from jcenter or maven central"},{"location":"server/#3-use-the-pre-configured-docker-image","text":"To build the docker images for the conductor server and ui run the commands: cd docker docker-compose build After the docker images are built, run the following command to start the containers: docker-compose up This will create a docker container network that consists of the following images: conductor:server, conductor:ui, elasticsearch:2.4 , and dynomite. To view the UI, navigate to localhost:5000 , to view the Swagger docs, navigate to localhost:8080 .","title":"3. Use the pre-configured Docker image"},{"location":"server/#configuration","text":"Conductor server uses a property file based configuration. The property file is passed to the Main class as a command line argument. java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path] log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).","title":"Configuration"},{"location":"server/#configuration-parameters","text":"# Database persistence model. Possible values are memory, redis, redis_cluster and dynomite. # If omitted, the persistence used is memory # # memory : The data is stored in memory and lost when the server dies. Useful for testing or demo # redis : non-Dynomite based redis instance # redis_cluster: AWS Elasticache Redis (cluster mode enabled).See [http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Clusters.Create.CON.RedisCluster.html] # redis_sentinel: Redis HA with Redis Sentinel. See [https://redis.io/topics/sentinel] # dynomite : Dynomite cluster. Use this for HA configuration. db=dynomite # Dynomite Cluster details. # format is host:port:rack separated by semicolon # for AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. The region in this case does not matter workflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e # Dynomite cluster name workflow.dynomite.cluster.name=dyno_cluster_name # Maximum connections to redis/dynomite workflow.dynomite.connection.maxConnsPerHost=31 # Namespace for the keys stored in Dynomite/Redis workflow.namespace.prefix=conductor # Namespace prefix for the dyno queues workflow.namespace.queue.prefix=conductor_queues # No. of threads allocated to dyno-queues (optional) queues.dynomite.threads=10 # Non-quorum port used to connect to local redis. Used by dyno-queues. # When using redis directly, set this to the same port as redis server. # For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite. queues.dynomite.nonQuorum.port=22122 # Transport address to elasticsearch # Specifying multiple node urls is not supported. specify one of the nodes' url, or a load balancer. workflow.elasticsearch.url=localhost:9300 # Name of the elasticsearch cluster workflow.elasticsearch.index.name=conductor # Additional modules (optional) conductor.additional.modules=class_extending_com.google.inject.AbstractModule","title":"Configuration Parameters"},{"location":"server/#high-availability-configuration","text":"Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs. The scalability of the server is achieved by scaling the Dynomite cluster along with dyno-queues which is used for queues. Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).","title":"High Availability Configuration"},{"location":"server/#using-standalone-redis-elasticache","text":"Conductor server can be used with a standlone Redis or ElastiCache server. To configure the server, change the config to use the following: db=redis # For AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. # The region in this case does not matter workflow.dynomite.cluster.hosts=server_address:server_port:us-east-1e workflow.dynomite.connection.maxConnsPerHost=31 queues.dynomite.nonQuorum.port=server_port","title":"Using Standalone Redis / ElastiCache"},{"location":"worker/","text":"Conductor tasks executed by remote workers communicates over HTTP endpoints to poll for the task and updates the status of the execution. Conductor provides a framework to poll for tasks, manage the execution thread and update the status of the execution back to the server. The framework provides libraries in Java and Python. Other language support can be added by using the HTTP endpoints for task management. Java \u00b6 Implement Worker interface to implement the task. Use WorkflowTaskCoordinator to register the worker(s) and initialize the polling loop. Sample Worker Implementation Example WorkflowTaskCoordinator \u00b6 Manages the Task workers thread pool and server communication (poll, task update and ack). Worker \u00b6 Property Description paused boolean. If set to true, the worker stops polling. pollCount No. of tasks to poll for. Used for batched polling. Each task is executed in a separate thread. longPollTimeout Time in millisecond for long polling to Conductor server for tasks These properties can be set either by Worker implementation or by setting the following system properties in the JVM: conductor.worker.<property> Applies to ALL the workers in the JVM conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property. Python \u00b6 https://github.com/Netflix/conductor/tree/dev/client/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is under development is not production battle tested. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Conductor Task Workers"},{"location":"worker/#java","text":"Implement Worker interface to implement the task. Use WorkflowTaskCoordinator to register the worker(s) and initialize the polling loop. Sample Worker Implementation Example","title":"Java"},{"location":"worker/#workflowtaskcoordinator","text":"Manages the Task workers thread pool and server communication (poll, task update and ack).","title":"WorkflowTaskCoordinator"},{"location":"worker/#worker","text":"Property Description paused boolean. If set to true, the worker stops polling. pollCount No. of tasks to poll for. Used for batched polling. Each task is executed in a separate thread. longPollTimeout Time in millisecond for long polling to Conductor server for tasks These properties can be set either by Worker implementation or by setting the following system properties in the JVM: conductor.worker.<property> Applies to ALL the workers in the JVM conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property.","title":"Worker"},{"location":"worker/#python","text":"https://github.com/Netflix/conductor/tree/dev/client/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is under development is not production battle tested. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Python"}]}