{
    "docs": [
        {
            "location": "/", 
            "text": "Conductor\n\n\n\n\n\n\nConductor is an \norchestration\n engine that runs in the cloud. \n\n\nMotivation\n\n\nWe built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:\n\n\n\n\nAllow creating complex process / business flows in which individual task is implemented by a microservice.\n\n\nA JSON DSL based blueprint defines the execution flow.\n\n\nProvide visibility and traceability into the these process flows.\n\n\nExpose control semantics around pause, resume, restart, etc allowing for better devops experience.\n\n\nAllow greater reuse of existing microservices providing an easier path for onboarding.\n\n\nUser interface to visualize the process flows.\n\n\nAbility to synchronously process all the tasks when needed.\n\n\nAbility to scale millions of concurrently running process flows.\n\n\nBacked by a queuing service abstracted from the clients.\n\n\nBe able to operate on HTTP or other transports e.g. gRPC.\n\n\n\n\nWhy not peer to peer choreography?\n\n\nWith peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:\n\n\n\n\nProcess flows are \u201cembedded\u201d within the code of multiple application.\n\n\nOften, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.\n\n\nAlmost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#conductor", 
            "text": "Conductor is an  orchestration  engine that runs in the cloud.", 
            "title": "Conductor"
        }, 
        {
            "location": "/#motivation", 
            "text": "We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:   Allow creating complex process / business flows in which individual task is implemented by a microservice.  A JSON DSL based blueprint defines the execution flow.  Provide visibility and traceability into the these process flows.  Expose control semantics around pause, resume, restart, etc allowing for better devops experience.  Allow greater reuse of existing microservices providing an easier path for onboarding.  User interface to visualize the process flows.  Ability to synchronously process all the tasks when needed.  Ability to scale millions of concurrently running process flows.  Backed by a queuing service abstracted from the clients.  Be able to operate on HTTP or other transports e.g. gRPC.   Why not peer to peer choreography?  With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:   Process flows are \u201cembedded\u201d within the code of multiple application.  Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.  Almost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Motivation"
        }, 
        {
            "location": "/intro/", 
            "text": "High Level Architecture\n\n\n\n\nThe API and storage layers are pluggable and provide ability to work with different backend and queue service providers.\n\n\nInstalling and Running\n\n\n\n\nRunning in production\n\n\nFor a detailed configuration guide on installing and running Conductor server in production visit \nConductor Server\n documentation.\n\n\n\n\nRunning In-Memory Server\n\n\nFollow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor.\n\n\n\n\nWarning\n\n\nIn-Memory server is meant for a quick demonstration purpose and does not store the data on disk.  All the data is lost once the server dies.\n\n\n\n\nCheckout the source from github\n\n\ngit clone git@github.com/Netflix/conductor.git\n\n\n\n\nStart Local Server\n\n\ncd server\n../gradlew server\n# wait for the server to come online\n\n\n\n\nSwagger APIs can be accessed at \nhttp://localhost:8080/\n\n\nStart UI Server\n\n\ncd ui\ngulp watch\n\n\n\n\nLaunch UI at \nhttp://localhost:3000/\n\n\n\n\nNote\n\n\nThe server will load a sample kitchen sink workflow definition by default.  See \nhere\n for details.\n\n\n\n\nRuntime Model\n\n\nConductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.\n\n\n\n\nNotes\n\n\n\n\nWorkers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers.\n\n\nTask Queues are used to schedule tasks for workers.  We use \ndyno-queues\n internally but it can easily be swapped with SQS or similar pub-sub mechanism.\n\n\nconductor-redis-persistence module uses \nDynomite\n for storing the state and metadata along with \nElasticsearch\n for indexing backend.\n\n\nSee section under extending backend for implementing support for different databases for storage and indexing.\n\n\n\n\nHigh Level Steps\n\n\nSteps required for a new workflow to be registered and get executed:\n\n\n\n\nDefine task definitions used by the workflow.\n\n\nCreate the workflow definition\n\n\nCreate task worker(s) that polls for scheduled tasks at regular interval\n\n\n\n\nTrigger Workflow Execution\n\n\nPOST /workflow/{name}\n{\n    ... //json payload as workflow input\n}\n\n\n\n\nPolling for a task\n\n\nGET /tasks/poll/batch/{taskType}\n\n\n\n\nUpdate task status\n\n\nPOST /tasks\n{\n    \noutputData\n: {\n        \nencodeResult\n:\nsuccess\n,\n        \nlocation\n: \nhttp://cdn.example.com/file/location.png\n\n        //any task specific output\n     },\n     \nstatus\n: \nCOMPLETED\n\n}", 
            "title": "Getting Started"
        }, 
        {
            "location": "/intro/#high-level-architecture", 
            "text": "The API and storage layers are pluggable and provide ability to work with different backend and queue service providers.", 
            "title": "High Level Architecture"
        }, 
        {
            "location": "/intro/#installing-and-running", 
            "text": "Running in production  For a detailed configuration guide on installing and running Conductor server in production visit  Conductor Server  documentation.", 
            "title": "Installing and Running"
        }, 
        {
            "location": "/intro/#running-in-memory-server", 
            "text": "Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor.   Warning  In-Memory server is meant for a quick demonstration purpose and does not store the data on disk.  All the data is lost once the server dies.", 
            "title": "Running In-Memory Server"
        }, 
        {
            "location": "/intro/#checkout-the-source-from-github", 
            "text": "git clone git@github.com/Netflix/conductor.git", 
            "title": "Checkout the source from github"
        }, 
        {
            "location": "/intro/#start-local-server", 
            "text": "cd server\n../gradlew server\n# wait for the server to come online  Swagger APIs can be accessed at  http://localhost:8080/", 
            "title": "Start Local Server"
        }, 
        {
            "location": "/intro/#start-ui-server", 
            "text": "cd ui\ngulp watch  Launch UI at  http://localhost:3000/   Note  The server will load a sample kitchen sink workflow definition by default.  See  here  for details.", 
            "title": "Start UI Server"
        }, 
        {
            "location": "/intro/#runtime-model", 
            "text": "Conductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.   Notes   Workers are remote systems and communicates over HTTP (or any supported RPC mechanism) with conductor servers.  Task Queues are used to schedule tasks for workers.  We use  dyno-queues  internally but it can easily be swapped with SQS or similar pub-sub mechanism.  conductor-redis-persistence module uses  Dynomite  for storing the state and metadata along with  Elasticsearch  for indexing backend.  See section under extending backend for implementing support for different databases for storage and indexing.", 
            "title": "Runtime Model"
        }, 
        {
            "location": "/intro/#high-level-steps", 
            "text": "Steps required for a new workflow to be registered and get executed:   Define task definitions used by the workflow.  Create the workflow definition  Create task worker(s) that polls for scheduled tasks at regular interval   Trigger Workflow Execution  POST /workflow/{name}\n{\n    ... //json payload as workflow input\n}  Polling for a task  GET /tasks/poll/batch/{taskType}  Update task status  POST /tasks\n{\n     outputData : {\n         encodeResult : success ,\n         location :  http://cdn.example.com/file/location.png \n        //any task specific output\n     },\n      status :  COMPLETED \n}", 
            "title": "High Level Steps"
        }, 
        {
            "location": "/intro/concepts/", 
            "text": "Workflow Definition\n\n\nWorkflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine.\n\nmore details\n\n\nTask Definition\n\n\n\n\nAll tasks need to be registered before they can be used by active workflows.\n\n\nA task can be re-used within multiple workflows.\nWorker tasks fall into two categories:\n\n\nSystem Task\n\n\nWorker Task\n\n\n\n\n\n\n\n\nSystem Tasks\n\n\nSystem tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability.\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nDYNAMIC\n\n\nA worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint\n\n\n\n\n\n\nDECIDE\n\n\nDecision tasks - implements case...switch style fork\n\n\n\n\n\n\nFORK\n\n\nForks a parallel set of tasks.  Each set is scheduled to be executed in parallel\n\n\n\n\n\n\nFORK_JOIN_DYNAMIC\n\n\nSimilar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task\n\n\n\n\n\n\nJOIN\n\n\nComplements FORK and FORK_JOIN_DYNAMIC.  Used to merge one of more parallel branches*\n\n\n\n\n\n\nSUB_WORKFLOW\n\n\nNest another workflow as a sub workflow task.  Upon execution it instantiates the sub workflow and awaits it completion\n\n\n\n\n\n\nEVENT\n\n\nProduces an event in a supported eventing system (e.g. Conductor, SQS)\n\n\n\n\n\n\n\n\nConductor provides an API to create user defined tasks that are executed in the same JVM as the engine.  see \nWorkflowSystemTask\n interface for details.\n\n\nWorker Tasks\n\n\nWorker tasks are implemented by application(s) and runs in a separate environment from Conductor.  The worker tasks can be implemented in any language.  These tasks talk to Conductor server via REST API endpoints to poll for tasks and update its status after execution.\n\n\nWorker tasks are identified by task type \nSIMPLE\n in the blueprint.\n\n\nLifecycle of a Workflow Task\n\n\n\n\nmore details", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/intro/concepts/#workflow-definition", 
            "text": "Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. more details", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/intro/concepts/#task-definition", 
            "text": "All tasks need to be registered before they can be used by active workflows.  A task can be re-used within multiple workflows.\nWorker tasks fall into two categories:  System Task  Worker Task", 
            "title": "Task Definition"
        }, 
        {
            "location": "/intro/concepts/#system-tasks", 
            "text": "System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability.     Name  Purpose      DYNAMIC  A worker task which is derived based on the input expression to the task, rather than being statically defined as part of the blueprint    DECIDE  Decision tasks - implements case...switch style fork    FORK  Forks a parallel set of tasks.  Each set is scheduled to be executed in parallel    FORK_JOIN_DYNAMIC  Similar to FORK, but rather than the set of tasks defined in the blueprint for parallel execution, FORK_JOIN_DYNAMIC spawns the parallel tasks based on the input expression to this task    JOIN  Complements FORK and FORK_JOIN_DYNAMIC.  Used to merge one of more parallel branches*    SUB_WORKFLOW  Nest another workflow as a sub workflow task.  Upon execution it instantiates the sub workflow and awaits it completion    EVENT  Produces an event in a supported eventing system (e.g. Conductor, SQS)     Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine.  see  WorkflowSystemTask  interface for details.", 
            "title": "System Tasks"
        }, 
        {
            "location": "/intro/concepts/#worker-tasks", 
            "text": "Worker tasks are implemented by application(s) and runs in a separate environment from Conductor.  The worker tasks can be implemented in any language.  These tasks talk to Conductor server via REST API endpoints to poll for tasks and update its status after execution.  Worker tasks are identified by task type  SIMPLE  in the blueprint.", 
            "title": "Worker Tasks"
        }, 
        {
            "location": "/intro/concepts/#lifecycle-of-a-workflow-task", 
            "text": "more details", 
            "title": "Lifecycle of a Workflow Task"
        }, 
        {
            "location": "/metadata/", 
            "text": "Task Definition\n\n\nConductor maintains a registry of worker task types.  A task type MUST be registered before using in a workflow.\n\n\nExample\n\n\n{\n  \nname\n: \nencode_task\n,\n  \nretryCount\n: 3,\n  \ntimeoutSeconds\n: 1200,\n  \ninputKeys\n: [\n    \nsourceRequestId\n,\n    \nqcElementType\n\n  ],\n  \noutputKeys\n: [\n    \nstate\n,\n    \nskipped\n,\n    \nresult\n\n  ],\n  \ntimeoutPolicy\n: \nTIME_OUT_WF\n,\n  \nretryLogic\n: \nFIXED\n,\n  \nretryDelaySeconds\n: 600,\n  \nresponseTimeoutSeconds\n: 3600\n}\n\n\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nTask Type\n\n\nUnique\n\n\n\n\n\n\nretryCount\n\n\nNo. of retries to attempt when a task is marked as failure\n\n\n\n\n\n\n\n\nretryLogic\n\n\nMechanism for the retries\n\n\nsee possible values below\n\n\n\n\n\n\ntimeoutSeconds\n\n\nTime in milliseconds, after which the task is marked as TIMED_OUT if not completed after transiting to \nIN_PROGRESS\n status\n\n\nNo timeouts if set to 0\n\n\n\n\n\n\ntimeoutPolicy\n\n\nTask's timeout policy\n\n\nsee possible values below\n\n\n\n\n\n\nresponseTimeoutSeconds\n\n\nif greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noutputKeys\n\n\nSet of keys of task's output.  Used for documenting task's output\n\n\n\n\n\n\n\n\n\n\nRetry Logic\n\n\n\n\nFIXED : Reschedule the task after the \nretryDelaySeconds\n\n\nEXPONENTIAL_BACKOFF : reschedule after \nretryDelaySeconds  * attempNo\n\n\n\n\nTimeout Policy\n\n\n\n\nRETRY : Retries the task again\n\n\nTIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated\n\n\nALERT_ONLY : Registers a counter (task_timeout)\n\n\n\n\nWorkflow Definition\n\n\nWorkflows are defined using a JSON based DSL.\n\n\nExample\n\n\n{\n  \nname\n: \nencode_and_deploy\n,\n  \ndescription\n: \nEncodes a file and deploys to CDN\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \nencode\n,\n      \ntaskReferenceName\n: \nencode\n,\n      \ntype\n: \nSIMPLE\n,\n      \ninputParameters\n: {\n        \nfileLocation\n: \n${workflow.input.fileLocation}\n\n      }\n    },\n    {\n      \nname\n: \ndeploy\n,\n      \ntaskReferenceName\n: \nd1\n,\n      \ntype\n: \nSIMPLE\n,\n      \ninputParameters\n: {\n        \nfileLocation\n: \n${encode.output.encodeLocation}\n\n      }\n\n    }\n  ],\n  \noutputParameters\n: {\n    \ncdn_url\n: \n${d1.output.location}\n\n  },\n  \nschemaVersion\n: 2\n}\n\n\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName of the workflow\n\n\n\n\n\n\n\n\ndescription\n\n\nDescriptive name of the workflow\n\n\n\n\n\n\n\n\nversion\n\n\nNumeric field used to identify the version of the schema.  Use incrementing numbers\n\n\nWhen starting a workflow execution, if not specified, the definition with highest version is used\n\n\n\n\n\n\ntasks\n\n\nAn array of task definitions as described below.\n\n\n\n\n\n\n\n\noutputParameters\n\n\nJSON template used to generate the output of the workflow\n\n\nIf not specified, the output is defined as the output of the \nlast\n executed task\n\n\n\n\n\n\ninputParameters\n\n\nList of input parameters.  Used for documenting the required inputs to workflow\n\n\noptional\n\n\n\n\n\n\n\n\nTasks within Workflow\n\n\ntasks\n property in a workflow defines an array of tasks to be executed in that order.\nBelow are the mandatory minimum parameters required for each task:\n\n\n\n\n\n\n\n\nfield\n\n\ndescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName of the task.  MUST be registered as a task type with Conductor before starting workflow\n\n\n\n\n\n\n\n\ntaskReferenceName\n\n\nAlias used to refer the task within the workflow.  MUST be unique.\n\n\n\n\n\n\n\n\ntype\n\n\nType of task. SIMPLE for tasks executed by remote workers, or one of the system task types\n\n\n\n\n\n\n\n\noptional\n\n\ntrue  or false.  When set to true - workflow continues even if the task fails.  The status of the task is reflected as \nCOMPLETED_WITH_ERRORS\n\n\nDefaults to \nfalse\n\n\n\n\n\n\ninputParameters\n\n\nJSON template that defines the input given to the task\n\n\nSee \"wiring inputs and outputs\" for details\n\n\n\n\n\n\n\n\nIn addition to these parameters, additional parameters specific to the task type are required as documented \nhere\n\n\nWiring Inputs and Outputs\n\n\nWorkflows are supplied inputs by client when a new execution is triggered. \nWorkflow input is a JSON payload that is available via \n${workflow.input...}\n expressions.\n\n\nEach task in the workflow is given input based on the \ninputParameters\n template configured in workflow definition.  \ninputParameters\n is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution.\n\n\nSyntax for mapping the values follows the pattern as: \n\n\n${SOURCE.input/output.JSONPath}\n\n\n\n\n\n\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\n\n\nSOURCE\n\n\ncan be either \"workflow\" or reference name of any of the task\n\n\n\n\n\n\ninput/output\n\n\nrefers to either the input or output of the source\n\n\n\n\n\n\nJSONPath\n\n\nJSON path expression to extract JSON fragment from source's input/output\n\n\n\n\n\n\n\n\n\n\nJSON Path Support\n\n\nConductor supports \nJSONPath\n specification and uses Java implementation from \nhere\n.\n\n\n\n\nExample\n\n\nConsider a task with input configured to use input/output parameters from workflow and a task named \nloc_task\n.\n\n\n{\n  \ninputParameters\n: {\n    \nmovieId\n: \n${workflow.input.movieId}\n,\n    \nurl\n: \n${workflow.input.fileLocation}\n,\n    \nlang\n: \n${loc_task.output.languages[0]}\n,\n    \nhttp_request\n: {\n      \nmethod\n: \nPOST\n,\n      \nurl\n: \nhttp://example.com/${loc_task.output.fileId}/encode\n,\n      \nbody\n: {\n        \nrecipe\n: \n${workflow.input.recipe}\n,\n        \nparams\n: {\n          \nwidth\n: 100,\n          \nheight\n: 100\n        }\n      },\n      \nheaders\n: {\n        \nAccept\n: \napplication/json\n,\n        \nContent-Type\n: \napplication/json\n\n      }\n    }\n  }\n}\n\n\n\n\nConsider the following as the \nworkflow input\n\n\n{\n  \nmovieId\n: \nmovie_123\n,\n  \nfileLocation\n:\ns3://moviebucket/file123\n,\n  \nrecipe\n:\npng\n\n}\n\n\n\n\nAnd the output of the \nloc_task\n as the following;\n\n\n{\n  \nfileId\n: \nfile_xxx_yyy_zzz\n,\n  \nlanguages\n: [\nen\n,\nja\n,\nes\n]\n}\n\n\n\n\nWhen scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows:\n\n\n{\n  \nmovieId\n: \nmovie_123\n,\n  \nurl\n: \ns3://moviebucket/file123\n,\n  \nlang\n: \nen\n,\n  \nhttp_request\n: {\n    \nmethod\n: \nPOST\n,\n    \nurl\n: \nhttp://example.com/file_xxx_yyy_zzz/encode\n,\n    \nbody\n: {\n      \nrecipe\n: \npng\n,\n      \nparams\n: {\n        \nwidth\n: 100,\n        \nheight\n: 100\n      }\n    },\n    \nheaders\n: {\n        \nAccept\n: \napplication/json\n,\n        \nContent-Type\n: \napplication/json\n\n    }\n  }\n}", 
            "title": "Metadata Definitions"
        }, 
        {
            "location": "/metadata/#task-definition", 
            "text": "Conductor maintains a registry of worker task types.  A task type MUST be registered before using in a workflow.  Example  {\n   name :  encode_task ,\n   retryCount : 3,\n   timeoutSeconds : 1200,\n   inputKeys : [\n     sourceRequestId ,\n     qcElementType \n  ],\n   outputKeys : [\n     state ,\n     skipped ,\n     result \n  ],\n   timeoutPolicy :  TIME_OUT_WF ,\n   retryLogic :  FIXED ,\n   retryDelaySeconds : 600,\n   responseTimeoutSeconds : 3600\n}     field  description  Notes      name  Task Type  Unique    retryCount  No. of retries to attempt when a task is marked as failure     retryLogic  Mechanism for the retries  see possible values below    timeoutSeconds  Time in milliseconds, after which the task is marked as TIMED_OUT if not completed after transiting to  IN_PROGRESS  status  No timeouts if set to 0    timeoutPolicy  Task's timeout policy  see possible values below    responseTimeoutSeconds  if greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.          outputKeys  Set of keys of task's output.  Used for documenting task's output      Retry Logic   FIXED : Reschedule the task after the  retryDelaySeconds  EXPONENTIAL_BACKOFF : reschedule after  retryDelaySeconds  * attempNo   Timeout Policy   RETRY : Retries the task again  TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated  ALERT_ONLY : Registers a counter (task_timeout)", 
            "title": "Task Definition"
        }, 
        {
            "location": "/metadata/#workflow-definition", 
            "text": "Workflows are defined using a JSON based DSL.  Example  {\n   name :  encode_and_deploy ,\n   description :  Encodes a file and deploys to CDN ,\n   version : 1,\n   tasks : [\n    {\n       name :  encode ,\n       taskReferenceName :  encode ,\n       type :  SIMPLE ,\n       inputParameters : {\n         fileLocation :  ${workflow.input.fileLocation} \n      }\n    },\n    {\n       name :  deploy ,\n       taskReferenceName :  d1 ,\n       type :  SIMPLE ,\n       inputParameters : {\n         fileLocation :  ${encode.output.encodeLocation} \n      }\n\n    }\n  ],\n   outputParameters : {\n     cdn_url :  ${d1.output.location} \n  },\n   schemaVersion : 2\n}     field  description  Notes      name  Name of the workflow     description  Descriptive name of the workflow     version  Numeric field used to identify the version of the schema.  Use incrementing numbers  When starting a workflow execution, if not specified, the definition with highest version is used    tasks  An array of task definitions as described below.     outputParameters  JSON template used to generate the output of the workflow  If not specified, the output is defined as the output of the  last  executed task    inputParameters  List of input parameters.  Used for documenting the required inputs to workflow  optional", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/metadata/#tasks-within-workflow", 
            "text": "tasks  property in a workflow defines an array of tasks to be executed in that order.\nBelow are the mandatory minimum parameters required for each task:     field  description  Notes      name  Name of the task.  MUST be registered as a task type with Conductor before starting workflow     taskReferenceName  Alias used to refer the task within the workflow.  MUST be unique.     type  Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types     optional  true  or false.  When set to true - workflow continues even if the task fails.  The status of the task is reflected as  COMPLETED_WITH_ERRORS  Defaults to  false    inputParameters  JSON template that defines the input given to the task  See \"wiring inputs and outputs\" for details     In addition to these parameters, additional parameters specific to the task type are required as documented  here", 
            "title": "Tasks within Workflow"
        }, 
        {
            "location": "/metadata/#wiring-inputs-and-outputs", 
            "text": "Workflows are supplied inputs by client when a new execution is triggered. \nWorkflow input is a JSON payload that is available via  ${workflow.input...}  expressions.  Each task in the workflow is given input based on the  inputParameters  template configured in workflow definition.   inputParameters  is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution.  Syntax for mapping the values follows the pattern as:   ${SOURCE.input/output.JSONPath}     -  -      SOURCE  can be either \"workflow\" or reference name of any of the task    input/output  refers to either the input or output of the source    JSONPath  JSON path expression to extract JSON fragment from source's input/output      JSON Path Support  Conductor supports  JSONPath  specification and uses Java implementation from  here .   Example  Consider a task with input configured to use input/output parameters from workflow and a task named  loc_task .  {\n   inputParameters : {\n     movieId :  ${workflow.input.movieId} ,\n     url :  ${workflow.input.fileLocation} ,\n     lang :  ${loc_task.output.languages[0]} ,\n     http_request : {\n       method :  POST ,\n       url :  http://example.com/${loc_task.output.fileId}/encode ,\n       body : {\n         recipe :  ${workflow.input.recipe} ,\n         params : {\n           width : 100,\n           height : 100\n        }\n      },\n       headers : {\n         Accept :  application/json ,\n         Content-Type :  application/json \n      }\n    }\n  }\n}  Consider the following as the  workflow input  {\n   movieId :  movie_123 ,\n   fileLocation : s3://moviebucket/file123 ,\n   recipe : png \n}  And the output of the  loc_task  as the following;  {\n   fileId :  file_xxx_yyy_zzz ,\n   languages : [ en , ja , es ]\n}  When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows:  {\n   movieId :  movie_123 ,\n   url :  s3://moviebucket/file123 ,\n   lang :  en ,\n   http_request : {\n     method :  POST ,\n     url :  http://example.com/file_xxx_yyy_zzz/encode ,\n     body : {\n       recipe :  png ,\n       params : {\n         width : 100,\n         height : 100\n      }\n    },\n     headers : {\n         Accept :  application/json ,\n         Content-Type :  application/json \n    }\n  }\n}", 
            "title": "Wiring Inputs and Outputs"
        }, 
        {
            "location": "/metadata/systask/", 
            "text": "Dynamic Task\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndynamicTaskNameParam\n\n\nName of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \nuser_task\n,\n  \ntaskReferenceName\n: \nt1\n,\n  \ninputParameters\n: {\n    \nfiles\n: \n${workflow.input.files}\n,\n    \ntaskToExecute\n: \n${workflow.input.user_supplied_task}\n\n  },\n  \ntype\n: \nDYNAMIC\n,\n  \ndynamicTaskNameParam\n: \ntaskToExecute\n\n}\n\n\n\n\nIf the workflow is started with input parameter user_supplied_task's value as \nuser_task_2\n, Conductor will schedule \nuser_task_2\n when scheduling this dynamic task.\n\n\nDecision\n\n\nA decision task is similar to \ncase...switch\n statement in a programming language.\nThe task takes 3 parameters:\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ncaseValueParam\n\n\nName of the parameter in task input whose value will be used as a switch.\n\n\n\n\n\n\ndecisionCases\n\n\nMap where key is possible values of \ncaseValueParam\n with value being list of tasks to be executed.\n\n\n\n\n\n\ndefaultCase\n\n\nList of tasks to be executed when no matching value if found in decision case (default condition)\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \ndecide_task\n,\n  \ntaskReferenceName\n: \ndecide1\n,\n  \ninputParameters\n: {\n    \ncase_value_param\n: \n${workflow.input.movieType}\n\n  },\n  \ntype\n: \nDECISION\n,\n  \ncaseValueParam\n: \ncase_value_param\n,\n  \ndecisionCases\n: {\n    \nShow\n: [\n      {\n        \nname\n: \nsetup_episodes\n,\n        \ntaskReferenceName\n: \nse1\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_episode_artwork\n,\n        \ntaskReferenceName\n: \nga\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ],\n    \nMovie\n: [\n      {\n        \nname\n: \nsetup_movie\n,\n        \ntaskReferenceName\n: \nsm\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_movie_artwork\n,\n        \ntaskReferenceName\n: \ngma\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ]\n  }\n}\n\n\n\n\nFork\n\n\nFork is used to schedule parallel set of tasks.\n\n\nParameters:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nforkTasks\n\n\nA list of list of tasks.  Each sublist is scheduled to be executed in parallel.  However, tasks within the sublists are scheduled in a serial fashion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nforkTasks\n: [\n    [\n      {\n        \nname\n: \ntask11\n,\n        \ntaskReferenceName\n: \nt11\n\n      },\n      {\n        \nname\n: \ntask12\n,\n        \ntaskReferenceName\n: \nt12\n\n      }\n    ],\n    [\n      {\n        \nname\n: \ntask21\n,\n        \ntaskReferenceName\n: \nt21\n\n      },\n      {\n        \nname\n: \ntask22\n,\n        \ntaskReferenceName\n: \nt22\n\n      }\n    ]\n  ]\n}\n\n\n\n\nWhen executed, \ntask11\n and \ntask21\n are scheduled to be executed at the same time.\n\n\nDynamic Fork\n\n\nA dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndynamicForkTasksParam\n\n\nName of the parameter that contains list of workflow task configuration to be executed in parallel\n\n\n\n\n\n\ndynamicForkTasksInputParamName\n\n\nName of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \ndynamicTasks\n: \n${taskA.output.dynamicTasksJSON}\n,\n  \ndynamicTasksInput\n: \n${taskA.output.dynamicTasksInputJSON}\n,\n  \ntype\n: \nFORK_JOIN_DYNAMIC\n,\n  \ndynamicForkTasksParam\n: \ndynamicTasks\n,\n  \ndynamicForkTasksInputParamName\n: \ndynamicTasksInput\n\n}\n\n\n\n\nConsider \ntaskA\n's output as:\n\n\n{\n  \ndynamicTasksInputJSON\n: {\n    \nforkedTask1\n: {\n      \nwidth\n: 100,\n      \nheight\n: 100,\n      \nparams\n: {\n        \nrecipe\n: \njpg\n\n      }\n    },\n    \nforkedTask2\n: {\n      \nwidth\n: 200,\n      \nheight\n: 200,\n      \nparams\n: {\n        \nrecipe\n: \njpg\n\n      }\n    }\n  },\n  \ndynamicTasksJSON\n: [\n    {\n      \nname\n: \nencode_task\n,\n      \ntaskReferenceName\n: \nforkedTask1\n,\n      \ntype\n: \nSIMPLE\n\n    },\n    {\n      \nname\n: \nencode_task\n,\n      \ntaskReferenceName\n: \nforkedTask2\n,\n      \ntype\n: \nSIMPLE\n\n    }\n  ]\n}\n\n\n\n\nWhen executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_\n\n\n\n\nDynamic Fork and Join\n\n\nA Join task MUST follow FORK_JOIN_DYNAMIC\n\n\nWorkflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task.  However, given the dynamic nature of the task, no joinOn parameters are required for this Join.  The join will wait for ALL the forked branches to complete before completing.\n\n\nUnlike FORK, which can execute parallel flows with each fork executing a series of tasks in  sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork.  However, forked task can be a Sub Workflow, allowing for more complex execution flows.\n\n\n\n\nJoin\n\n\nJoin task is used to wait for completion of one or more tasks spawned by fork tasks.\n\n\nParameters\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\njoinOn\n\n\nList of task reference name, for which the JOIN will wait for completion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n    \njoinOn\n: [\ntaskRef1\n, \ntaskRef3\n]\n}\n\n\n\n\nJoin Task Output\n\n\nFork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.\n\n\nSub Workflow\n\n\nSub Workflow task allows for nesting a workflow within another workflow.\n\n\nParameters\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsubWorkflowParam\n\n\nList of task reference name, for which the JOIN will wait for completion.\n\n\n\n\n\n\n\n\nExample\n\n\n{\n  \nname\n: \nsub_workflow_task\n,\n  \ntaskReferenceName\n: \nsub1\n,\n  \ninputParameters\n: {\n    \nrequestId\n: \n${workflow.input.requestId}\n,\n    \nfile\n: \n${encode.output.location}\n\n  },\n  \ntype\n: \nSUB_WORKFLOW\n,\n  \nsubWorkflowParam\n: {\n    \nname\n: \ndeployment_workflow\n,\n    \nversion\n: 1\n  }\n}\n\n\n\n\nWhen executed, a \ndeployment_workflow\n is executed with two input parameters requestId and \nfile\n.  The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. \n\n\nWait\n\n\nA wait task is implemented as a gate that remains in \nIN_PROGRESS\n state unless marked as \nCOMPLETED\n or \nFAILED\n by an external trigger.\nTo use a wait task, set the task type as \nWAIT\n\n\nParameters\n\n\nNone required.\n\n\nExternal Triggers for Wait Task\n\n\nTask Resource endpoint can be used to update the status of a task to a terminate state. \n\n\nContrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as \nCOMPLETED\n or \nFAILED\n.  \n\n\nSQS Queues\n\n\n\n\nSQS queues used by the server to update the task status can be retrieve using the following API:\n\n\n\n\nGET /queue\n\n\n\n\n\n\nWhen updating the status of the task, the message needs to conform to the following spec:\n\n\nMessage has to be a valid JSON string.\n\n\nThe message JSON should contain a key named \nexternalId\n with the value being a JSONified string that contains the following keys:\n\n\nworkflowId\n: Id of the workflow\n\n\ntaskRefName\n: Task reference name that should be updated.\n\n\n\n\n\n\nEach queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a \nCOMPLETED\n queue marks the task status as \nCOMPLETED\n.\n\n\nTasks' output is updated with the message.\n\n\n\n\n\n\n\n\nExample SQS Payload:\n\n\n{\n  \nsome_key\n: \nvaluex\n,\n  \nexternalId\n: \n{\\\ntaskRefName\\\n:\\\nTASK_REFERENCE_NAME\\\n,\\\nworkflowId\\\n:\\\nWORKFLOW_ID\\\n}\n\n}\n\n\n\n\nHTTP\n\n\nAn HTTP task is used to make calls to another microservice over HTTP.\n\n\nParameters\n\n\nThe task expects an input parameter named \nhttp_request\n as part of the task's input with the following details:\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nuri\n\n\nURI for the service.  Can be a partial when using vipAddress or includes the server address.\n\n\n\n\n\n\nmethod\n\n\nHTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD\n\n\n\n\n\n\naccept\n\n\nAccept header as required by server.\n\n\n\n\n\n\ncontentType\n\n\nContent Type - supported types are text/plain, text/html and, application/json\n\n\n\n\n\n\nheaders\n\n\nA map of additional http headers to be sent along with the request.\n\n\n\n\n\n\nbody\n\n\nRequest body\n\n\n\n\n\n\nvipAddress\n\n\nWhen using discovery based service URLs.\n\n\n\n\n\n\n\n\nHTTP Task Output\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nresponse\n\n\nJSON body containing the response if one is present\n\n\n\n\n\n\nheaders\n\n\nResponse Headers\n\n\n\n\n\n\nstatusCode\n\n\nInteger status code\n\n\n\n\n\n\n\n\nExample\n\n\nTask Input payload using vipAddress\n\n\n{\n  \nhttp_request\n: {\n    \nvipAddress\n: \nexamplevip-prod\n,\n    \nuri\n: \n/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nTask Input using an absolute URL\n\n\n{\n  \nhttp_request\n: {\n    \nuri\n: \nhttp://example.com/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nThe task is marked as \nFAILED\n if the request cannot be completed or the remote server returns non successful status code. \n\n\n\n\nNote\n\n\nHTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response.  XML input/output is currently not supported.  However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.\n\n\n\n\nEvent\n\n\nEvent task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS.  Event tasks are useful for creating event based dependencies for workflows and tasks.\n\n\nParameters\n\n\n\n\n\n\n\n\nname\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsink\n\n\nQualified name of the event that is produced.  e.g. conductor or sqs:sqs_queue_name\n\n\n\n\n\n\n\n\nExample\n\n\n{\n    \nsink\n: 'sqs:example_sqs_queue_name'\n}\n\n\n\n\nWhen producing an event with Conductor as sink, the event name follows the structure:\n\nconductor:\nworkflow_name\n:\ntask_reference_name\n\n\nFor SQS, use the \nname\n of the queue and NOT the URI.  Conductor looks up the URI based on the name.\n\n\n\n\nWarning\n\n\nWhen using SQS add the \nContribsModule\n to the deployment.  The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs.\n\n\n\n\nSupported Sinks\n\n\n\n\nConductor\n\n\nSQS\n\n\n\n\nEvent Task Input\n\n\nThe input given to the event task is made available to the published message as payload.  e.g. if a message is put into SQS queue (sink is sqs) then the message payload will be the input to the task.\n\n\nEvent Task Output\n\n\nevent_produced\n Name of the event produced.", 
            "title": "System Tasks"
        }, 
        {
            "location": "/metadata/systask/#dynamic-task", 
            "text": "", 
            "title": "Dynamic Task"
        }, 
        {
            "location": "/metadata/systask/#parameters", 
            "text": "name  description      dynamicTaskNameParam  Name of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example", 
            "text": "{\n   name :  user_task ,\n   taskReferenceName :  t1 ,\n   inputParameters : {\n     files :  ${workflow.input.files} ,\n     taskToExecute :  ${workflow.input.user_supplied_task} \n  },\n   type :  DYNAMIC ,\n   dynamicTaskNameParam :  taskToExecute \n}  If the workflow is started with input parameter user_supplied_task's value as  user_task_2 , Conductor will schedule  user_task_2  when scheduling this dynamic task.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#decision", 
            "text": "A decision task is similar to  case...switch  statement in a programming language.\nThe task takes 3 parameters:", 
            "title": "Decision"
        }, 
        {
            "location": "/metadata/systask/#parameters_1", 
            "text": "name  description      caseValueParam  Name of the parameter in task input whose value will be used as a switch.    decisionCases  Map where key is possible values of  caseValueParam  with value being list of tasks to be executed.    defaultCase  List of tasks to be executed when no matching value if found in decision case (default condition)", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example_1", 
            "text": "{\n   name :  decide_task ,\n   taskReferenceName :  decide1 ,\n   inputParameters : {\n     case_value_param :  ${workflow.input.movieType} \n  },\n   type :  DECISION ,\n   caseValueParam :  case_value_param ,\n   decisionCases : {\n     Show : [\n      {\n         name :  setup_episodes ,\n         taskReferenceName :  se1 ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_episode_artwork ,\n         taskReferenceName :  ga ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ],\n     Movie : [\n      {\n         name :  setup_movie ,\n         taskReferenceName :  sm ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_movie_artwork ,\n         taskReferenceName :  gma ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ]\n  }\n}", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#fork", 
            "text": "Fork is used to schedule parallel set of tasks.", 
            "title": "Fork"
        }, 
        {
            "location": "/metadata/systask/#parameters_2", 
            "text": "name  description      forkTasks  A list of list of tasks.  Each sublist is scheduled to be executed in parallel.  However, tasks within the sublists are scheduled in a serial fashion.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/metadata/systask/#example_2", 
            "text": "{\n   forkTasks : [\n    [\n      {\n         name :  task11 ,\n         taskReferenceName :  t11 \n      },\n      {\n         name :  task12 ,\n         taskReferenceName :  t12 \n      }\n    ],\n    [\n      {\n         name :  task21 ,\n         taskReferenceName :  t21 \n      },\n      {\n         name :  task22 ,\n         taskReferenceName :  t22 \n      }\n    ]\n  ]\n}  When executed,  task11  and  task21  are scheduled to be executed at the same time.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#dynamic-fork", 
            "text": "A dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.     name  description      dynamicForkTasksParam  Name of the parameter that contains list of workflow task configuration to be executed in parallel    dynamicForkTasksInputParamName  Name of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task", 
            "title": "Dynamic Fork"
        }, 
        {
            "location": "/metadata/systask/#example_3", 
            "text": "{\n   dynamicTasks :  ${taskA.output.dynamicTasksJSON} ,\n   dynamicTasksInput :  ${taskA.output.dynamicTasksInputJSON} ,\n   type :  FORK_JOIN_DYNAMIC ,\n   dynamicForkTasksParam :  dynamicTasks ,\n   dynamicForkTasksInputParamName :  dynamicTasksInput \n}  Consider  taskA 's output as:  {\n   dynamicTasksInputJSON : {\n     forkedTask1 : {\n       width : 100,\n       height : 100,\n       params : {\n         recipe :  jpg \n      }\n    },\n     forkedTask2 : {\n       width : 200,\n       height : 200,\n       params : {\n         recipe :  jpg \n      }\n    }\n  },\n   dynamicTasksJSON : [\n    {\n       name :  encode_task ,\n       taskReferenceName :  forkedTask1 ,\n       type :  SIMPLE \n    },\n    {\n       name :  encode_task ,\n       taskReferenceName :  forkedTask2 ,\n       type :  SIMPLE \n    }\n  ]\n}  When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_   Dynamic Fork and Join  A Join task MUST follow FORK_JOIN_DYNAMIC  Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task.  However, given the dynamic nature of the task, no joinOn parameters are required for this Join.  The join will wait for ALL the forked branches to complete before completing.  Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in  sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork.  However, forked task can be a Sub Workflow, allowing for more complex execution flows.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#join", 
            "text": "Join task is used to wait for completion of one or more tasks spawned by fork tasks.", 
            "title": "Join"
        }, 
        {
            "location": "/metadata/systask/#parameters_3", 
            "text": "name  description      joinOn  List of task reference name, for which the JOIN will wait for completion.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#example_4", 
            "text": "{\n     joinOn : [ taskRef1 ,  taskRef3 ]\n}", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#join-task-output", 
            "text": "Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.", 
            "title": "Join Task Output"
        }, 
        {
            "location": "/metadata/systask/#sub-workflow", 
            "text": "Sub Workflow task allows for nesting a workflow within another workflow.", 
            "title": "Sub Workflow"
        }, 
        {
            "location": "/metadata/systask/#parameters_4", 
            "text": "name  description      subWorkflowParam  List of task reference name, for which the JOIN will wait for completion.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#example_5", 
            "text": "{\n   name :  sub_workflow_task ,\n   taskReferenceName :  sub1 ,\n   inputParameters : {\n     requestId :  ${workflow.input.requestId} ,\n     file :  ${encode.output.location} \n  },\n   type :  SUB_WORKFLOW ,\n   subWorkflowParam : {\n     name :  deployment_workflow ,\n     version : 1\n  }\n}  When executed, a  deployment_workflow  is executed with two input parameters requestId and  file .  The task is marked as completed upon the completion of the spawned workflow.  If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#wait", 
            "text": "A wait task is implemented as a gate that remains in  IN_PROGRESS  state unless marked as  COMPLETED  or  FAILED  by an external trigger.\nTo use a wait task, set the task type as  WAIT", 
            "title": "Wait"
        }, 
        {
            "location": "/metadata/systask/#parameters_5", 
            "text": "None required.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#external-triggers-for-wait-task", 
            "text": "Task Resource endpoint can be used to update the status of a task to a terminate state.   Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as  COMPLETED  or  FAILED .", 
            "title": "External Triggers for Wait Task"
        }, 
        {
            "location": "/metadata/systask/#sqs-queues", 
            "text": "SQS queues used by the server to update the task status can be retrieve using the following API:   GET /queue   When updating the status of the task, the message needs to conform to the following spec:  Message has to be a valid JSON string.  The message JSON should contain a key named  externalId  with the value being a JSONified string that contains the following keys:  workflowId : Id of the workflow  taskRefName : Task reference name that should be updated.    Each queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a  COMPLETED  queue marks the task status as  COMPLETED .  Tasks' output is updated with the message.", 
            "title": "SQS Queues"
        }, 
        {
            "location": "/metadata/systask/#example-sqs-payload", 
            "text": "{\n   some_key :  valuex ,\n   externalId :  {\\ taskRefName\\ :\\ TASK_REFERENCE_NAME\\ ,\\ workflowId\\ :\\ WORKFLOW_ID\\ } \n}", 
            "title": "Example SQS Payload:"
        }, 
        {
            "location": "/metadata/systask/#http", 
            "text": "An HTTP task is used to make calls to another microservice over HTTP.", 
            "title": "HTTP"
        }, 
        {
            "location": "/metadata/systask/#parameters_6", 
            "text": "The task expects an input parameter named  http_request  as part of the task's input with the following details:     name  description      uri  URI for the service.  Can be a partial when using vipAddress or includes the server address.    method  HTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD    accept  Accept header as required by server.    contentType  Content Type - supported types are text/plain, text/html and, application/json    headers  A map of additional http headers to be sent along with the request.    body  Request body    vipAddress  When using discovery based service URLs.", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#http-task-output", 
            "text": "name  description      response  JSON body containing the response if one is present    headers  Response Headers    statusCode  Integer status code", 
            "title": "HTTP Task Output"
        }, 
        {
            "location": "/metadata/systask/#example_6", 
            "text": "Task Input payload using vipAddress  {\n   http_request : {\n     vipAddress :  examplevip-prod ,\n     uri :  / ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  Task Input using an absolute URL  {\n   http_request : {\n     uri :  http://example.com/ ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  The task is marked as  FAILED  if the request cannot be completed or the remote server returns non successful status code.    Note  HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response.  XML input/output is currently not supported.  However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#event", 
            "text": "Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS.  Event tasks are useful for creating event based dependencies for workflows and tasks.", 
            "title": "Event"
        }, 
        {
            "location": "/metadata/systask/#parameters_7", 
            "text": "name  description      sink  Qualified name of the event that is produced.  e.g. conductor or sqs:sqs_queue_name", 
            "title": "Parameters"
        }, 
        {
            "location": "/metadata/systask/#example_7", 
            "text": "{\n     sink : 'sqs:example_sqs_queue_name'\n}  When producing an event with Conductor as sink, the event name follows the structure: conductor: workflow_name : task_reference_name  For SQS, use the  name  of the queue and NOT the URI.  Conductor looks up the URI based on the name.   Warning  When using SQS add the  ContribsModule  to the deployment.  The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs.", 
            "title": "Example"
        }, 
        {
            "location": "/metadata/systask/#supported-sinks", 
            "text": "Conductor  SQS", 
            "title": "Supported Sinks"
        }, 
        {
            "location": "/metadata/systask/#event-task-input", 
            "text": "The input given to the event task is made available to the published message as payload.  e.g. if a message is put into SQS queue (sink is sqs) then the message payload will be the input to the task.", 
            "title": "Event Task Input"
        }, 
        {
            "location": "/metadata/systask/#event-task-output", 
            "text": "event_produced  Name of the event produced.", 
            "title": "Event Task Output"
        }, 
        {
            "location": "/events/", 
            "text": "Introduction\n\n\nEventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems.\n\n\nThis includes:\n\n\n\n\nBeing able to produce an event (message) in an external system like SQS or internal to Conductor. \n\n\nStart a workflow when a specific event occurs that matches the provided criteria.\n\n\n\n\nConductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow.  Eventing supports provides similar capability without explicitly adding dependencies and provides \nfire-and-forget\n style integrations.\n\n\nEvent Task\n\n\nEvent task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.\n\n\nSee \nEvent Task\n for documentation.\n\n\nEvent Handler\n\n\nEvent handlers are listeners registered that executes an action when a matching event occurs.  The supported actions are:\n\n\n\n\nStart a Workflow\n\n\nFail a Task\n\n\nComplete a Task\n\n\n\n\nEvent Handlers can be configured to listen to Conductor Events or an external event like SQS.\n\n\nConfiguration\n\n\nEvent Handlers are configured via \n/event/\n APIs.\n\n\nStructure:\n\n\n{\n  \nname\n : \ndescriptive unique name\n,\n  \nevent\n: \nevent_type:event_location\n,\n  \ncondition\n: \nboolean condition\n,\n  \nactions\n: [\nsee examples below\n]\n}\n\n\n\n\nCondition\n\n\nCondition is an expression that MUST evaluate to a boolean value.  A Javascript like syntax is supported that can be used to evaluate condition based on the payload.\nActions are executed only when the condition evaluates to \ntrue\n.\n\n\nExamples\n\n\nGiven the following payload in the message:\n\n\n{\n    \nfileType\n: \nAUDIO\n,\n    \nversion\n: 3,\n    \nmetadata\n: {\n       length: 300,\n       codec: \naac\n\n    }\n}\n\n\n\n\n\n\n\n\n\n\nExpression\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n$.version \n 1\n\n\ntrue\n\n\n\n\n\n\n$.version \n 10\n\n\nfalse\n\n\n\n\n\n\n$.metadata.length == 300\n\n\ntrue\n\n\n\n\n\n\n\n\nActions\n\n\nStart A Workflow\n\n\n{\n    \naction\n: \nstart_workflow\n,\n    \nstart_workflow\n: {\n        \nname\n: \nWORKFLOW_NAME\n,\n        \nversion\n: \noptional\n\n        \ninput\n: {\n            \nparam1\n: \n${param1}\n \n        }\n    }\n}\n\n\n\n\nComplete Task\n*\n\n\n{\n    \naction\n: \ncomplete_task\n,\n    \ncomplete_task\n: {\n      \nworkflowId\n: \n${source.externalId.workflowId}\n,\n      \ntaskRefName\n: \ntask_1\n,\n      \noutput\n: {\n        \nresponse\n: \n${source.result}\n\n      }\n    },\n    \nexpandInlineJSON\n: true\n}\n\n\n\n\nFail Task\n*\n\n\n{\n    \naction\n: \nfail_task\n,\n    \nfail_task\n: {\n      \nworkflowId\n: \n${source.externalId.workflowId}\n,\n      \ntaskRefName\n: \ntask_1\n,\n      \noutput\n: {\n        \nresponse\n: \n${source.result}\n\n      }\n    },\n    \nexpandInlineJSON\n: true\n}\n\n\n\n\nInput for starting a workflow and output when completing / failing task follows the same \nexpressions\n used for wiring workflow inputs.\n\n\n\n\nExpanding stringified JSON elements in payload\n\n\nexpandInlineJSON\n property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document.\n\nThis feature allows such elements to be used with JSON path expressions. \n\n\n\n\nExtending\n\n\nProvide the implementation of \nEventQueueProvider\n.\n\n\nSQS Queue Provider: \n\nSQSEventQueueProvider.java", 
            "title": "Event Handlers"
        }, 
        {
            "location": "/events/#introduction", 
            "text": "Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems.  This includes:   Being able to produce an event (message) in an external system like SQS or internal to Conductor.   Start a workflow when a specific event occurs that matches the provided criteria.   Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow.  Eventing supports provides similar capability without explicitly adding dependencies and provides  fire-and-forget  style integrations.", 
            "title": "Introduction"
        }, 
        {
            "location": "/events/#event-task", 
            "text": "Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks.  See  Event Task  for documentation.", 
            "title": "Event Task"
        }, 
        {
            "location": "/events/#event-handler", 
            "text": "Event handlers are listeners registered that executes an action when a matching event occurs.  The supported actions are:   Start a Workflow  Fail a Task  Complete a Task   Event Handlers can be configured to listen to Conductor Events or an external event like SQS.", 
            "title": "Event Handler"
        }, 
        {
            "location": "/events/#configuration", 
            "text": "Event Handlers are configured via  /event/  APIs.", 
            "title": "Configuration"
        }, 
        {
            "location": "/events/#structure", 
            "text": "{\n   name  :  descriptive unique name ,\n   event :  event_type:event_location ,\n   condition :  boolean condition ,\n   actions : [ see examples below ]\n}", 
            "title": "Structure:"
        }, 
        {
            "location": "/events/#condition", 
            "text": "Condition is an expression that MUST evaluate to a boolean value.  A Javascript like syntax is supported that can be used to evaluate condition based on the payload.\nActions are executed only when the condition evaluates to  true .  Examples  Given the following payload in the message:  {\n     fileType :  AUDIO ,\n     version : 3,\n     metadata : {\n       length: 300,\n       codec:  aac \n    }\n}     Expression  Result      $.version   1  true    $.version   10  false    $.metadata.length == 300  true", 
            "title": "Condition"
        }, 
        {
            "location": "/events/#actions", 
            "text": "Start A Workflow  {\n     action :  start_workflow ,\n     start_workflow : {\n         name :  WORKFLOW_NAME ,\n         version :  optional \n         input : {\n             param1 :  ${param1}  \n        }\n    }\n}  Complete Task *  {\n     action :  complete_task ,\n     complete_task : {\n       workflowId :  ${source.externalId.workflowId} ,\n       taskRefName :  task_1 ,\n       output : {\n         response :  ${source.result} \n      }\n    },\n     expandInlineJSON : true\n}  Fail Task *  {\n     action :  fail_task ,\n     fail_task : {\n       workflowId :  ${source.externalId.workflowId} ,\n       taskRefName :  task_1 ,\n       output : {\n         response :  ${source.result} \n      }\n    },\n     expandInlineJSON : true\n}  Input for starting a workflow and output when completing / failing task follows the same  expressions  used for wiring workflow inputs.   Expanding stringified JSON elements in payload  expandInlineJSON  property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. \nThis feature allows such elements to be used with JSON path expressions.", 
            "title": "Actions"
        }, 
        {
            "location": "/events/#extending", 
            "text": "Provide the implementation of  EventQueueProvider .  SQS Queue Provider:  SQSEventQueueProvider.java", 
            "title": "Extending"
        }, 
        {
            "location": "/metadata/kitchensink/", 
            "text": "An example kitchensink workflow that demonstrates the usage of all the schema constructs.\n\n\nDefinition\n\n\n{\n  \nname\n: \nkitchensink\n,\n  \ndescription\n: \nkitchensink workflow\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \ntask_1\n,\n      \ntaskReferenceName\n: \ntask_1\n,\n      \ninputParameters\n: {\n        \nmod\n: \n${workflow.input.mod}\n,\n        \noddEven\n: \n${workflow.input.oddEven}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    },\n    {\n      \nname\n: \nevent_task\n,\n      \ntaskReferenceName\n: \nevent_0\n,\n      \ninputParameters\n: {\n        \nmod\n: \n${workflow.input.mod}\n,\n        \noddEven\n: \n${workflow.input.oddEven}\n\n      },\n      \ntype\n: \nEVENT\n,\n      \nsink\n: \nconductor\n\n    },\n    {\n      \nname\n: \ndyntask\n,\n      \ntaskReferenceName\n: \ntask_2\n,\n      \ninputParameters\n: {\n        \ntaskToExecute\n: \n${workflow.input.task2Name}\n\n      },\n      \ntype\n: \nDYNAMIC\n,\n      \ndynamicTaskNameParam\n: \ntaskToExecute\n\n    },\n    {\n      \nname\n: \noddEvenDecision\n,\n      \ntaskReferenceName\n: \noddEvenDecision\n,\n      \ninputParameters\n: {\n        \noddEven\n: \n${task_2.output.oddEven}\n\n      },\n      \ntype\n: \nDECISION\n,\n      \ncaseValueParam\n: \noddEven\n,\n      \ndecisionCases\n: {\n        \n0\n: [\n          {\n            \nname\n: \ntask_4\n,\n            \ntaskReferenceName\n: \ntask_4\n,\n            \ninputParameters\n: {\n              \nmod\n: \n${task_2.output.mod}\n,\n              \noddEven\n: \n${task_2.output.oddEven}\n\n            },\n            \ntype\n: \nSIMPLE\n\n          },\n          {\n            \nname\n: \ndynamic_fanout\n,\n            \ntaskReferenceName\n: \nfanout1\n,\n            \ninputParameters\n: {\n              \ndynamicTasks\n: \n${task_4.output.dynamicTasks}\n,\n              \ninput\n: \n${task_4.output.inputs}\n\n            },\n            \ntype\n: \nFORK_JOIN_DYNAMIC\n,\n            \ndynamicForkTasksParam\n: \ndynamicTasks\n,\n            \ndynamicForkTasksInputParamName\n: \ninput\n\n          },\n          {\n            \nname\n: \ndynamic_join\n,\n            \ntaskReferenceName\n: \njoin1\n,\n            \ntype\n: \nJOIN\n\n          }\n        ],\n        \n1\n: [\n          {\n            \nname\n: \nfork_join\n,\n            \ntaskReferenceName\n: \nforkx\n,\n            \ntype\n: \nFORK_JOIN\n,\n            \nforkTasks\n: [\n              [\n                {\n                  \nname\n: \ntask_10\n,\n                  \ntaskReferenceName\n: \ntask_10\n,\n                  \ntype\n: \nSIMPLE\n\n                },\n                {\n                  \nname\n: \nsub_workflow_x\n,\n                  \ntaskReferenceName\n: \nwf3\n,\n                  \ninputParameters\n: {\n                    \nmod\n: \n${task_1.output.mod}\n,\n                    \noddEven\n: \n${task_1.output.oddEven}\n\n                  },\n                  \ntype\n: \nSUB_WORKFLOW\n,\n                  \nsubWorkflowParam\n: {\n                    \nname\n: \nsub_flow_1\n,\n                    \nversion\n: 1\n                  }\n                }\n              ],\n              [\n                {\n                  \nname\n: \ntask_11\n,\n                  \ntaskReferenceName\n: \ntask_11\n,\n                  \ntype\n: \nSIMPLE\n\n                },\n                {\n                  \nname\n: \nsub_workflow_x\n,\n                  \ntaskReferenceName\n: \nwf4\n,\n                  \ninputParameters\n: {\n                    \nmod\n: \n${task_1.output.mod}\n,\n                    \noddEven\n: \n${task_1.output.oddEven}\n\n                  },\n                  \ntype\n: \nSUB_WORKFLOW\n,\n                  \nsubWorkflowParam\n: {\n                    \nname\n: \nsub_flow_1\n,\n                    \nversion\n: 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n            \nname\n: \njoin\n,\n            \ntaskReferenceName\n: \njoin2\n,\n            \ntype\n: \nJOIN\n,\n            \njoinOn\n: [\n              \nwf3\n,\n              \nwf4\n\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \nname\n: \nsearch_elasticsearch\n,\n      \ntaskReferenceName\n: \nget_es_1\n,\n      \ninputParameters\n: {\n        \nhttp_request\n: {\n          \nuri\n: \nhttp://localhost:9200/conductor/_search?size=10\n,\n          \nmethod\n: \nGET\n\n        }\n      },\n      \ntype\n: \nHTTP\n\n    },\n    {\n      \nname\n: \ntask_30\n,\n      \ntaskReferenceName\n: \ntask_30\n,\n      \ninputParameters\n: {\n        \nstatuses\n: \n${get_es_1.output..status}\n,\n        \nworkflowIds\n: \n${get_es_1.output..workflowId}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    }\n  ],\n  \noutputParameters\n: {\n    \nstatues\n: \n${get_es_1.output..status}\n,\n    \nworkflowIds\n: \n${get_es_1.output..workflowId}\n\n  },\n  \nschemaVersion\n: 2\n}\n\n\n\n\nVisual Flow\n\n\n\n\nRunning Kitchensink Workflow\n\n\n\n\nStart the server as documented \nhere\n.  Use \n-DloadSample=true\n java system property when launching the server.  This will create a kitchensink workflow, related task definition and kick off an instance of kitchensink workflow.\n\n\nOnce the workflow has started, the first task remains in the \nSCHEDULED\n state.  This is because no workers are currently polling for the task.\n\n\nWe will use the REST endpoints directly to poll for tasks and updating the status.\n\n\n\n\nStart workflow execution\n\n\nStart the execution of the kitchensink workflow by posting the following:\n\n\ncurl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d '\n{\n    \ntask2Name\n: \ntask_5\n \n}\n'\n\n\n\n\nThe response is a text string identifying the workflow instance id.\n\n\nPoll for the first task:\n\n\ncurl http://localhost:8080/api/tasks/poll/task_1\n\n\n\n\nThe response should look something like:\n\n\n{\n    \ntaskType\n: \ntask_1\n,\n    \nstatus\n: \nIN_PROGRESS\n,\n    \ninputData\n: {\n        \nmod\n: null,\n        \noddEven\n: null\n    },\n    \nreferenceTaskName\n: \ntask_1\n,\n    \nretryCount\n: 0,\n    \nseq\n: 1,\n    \npollCount\n: 1,\n    \ntaskDefName\n: \ntask_1\n,\n    \nscheduledTime\n: 1486580932471,\n    \nstartTime\n: 1486580933869,\n    \nendTime\n: 0,\n    \nupdateTime\n: 1486580933902,\n    \nstartDelayInSeconds\n: 0,\n    \nretried\n: false,\n    \ncallbackFromWorker\n: true,\n    \nresponseTimeoutSeconds\n: 3600,\n    \nworkflowInstanceId\n: \nb0d1a935-3d74-46fd-92b2-0ca1e388659f\n,\n    \ntaskId\n: \nb9eea7dd-3fbd-46b9-a9ff-b00279459476\n,\n    \ncallbackAfterSeconds\n: 0,\n    \npolledTime\n: 1486580933902,\n    \nqueueWaitTime\n: 1398\n}\n\n\n\n\nUpdate the task status\n\n\n\n\nNote the values for \ntaskId\n and \nworkflowInstanceId\n fields from the poll response\n\n\nUpdate the status of the task as \nCOMPLETED\n as below:\n\n\n\n\ncurl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d '\n{\n    \ntaskId\n: \nb9eea7dd-3fbd-46b9-a9ff-b00279459476\n,\n    \nworkflowInstanceId\n: \nb0d1a935-3d74-46fd-92b2-0ca1e388659f\n,\n    \nstatus\n: \nCOMPLETED\n,\n    \noutput\n: {\n        \nmod\n: 5,\n        \ntaskToExecute\n: \ntask_1\n,\n        \noddEven\n: 0,\n        \ndynamicTasks\n: [\n            {\n                \nname\n: \ntask_1\n,\n                \ntaskReferenceName\n: \ntask_1_1\n,\n                \ntype\n: \nSIMPLE\n\n            },\n            {\n                \nname\n: \nsub_workflow_4\n,\n                \ntaskReferenceName\n: \nwf_dyn\n,\n                \ntype\n: \nSUB_WORKFLOW\n,\n                \nsubWorkflowParam\n: {\n                    \nname\n: \nsub_flow_1\n\n                }\n            }\n        ],\n        \ninputs\n: {\n            \ntask_1_1\n: {},\n            \nwf_dyn\n: {}\n        }\n    }\n}'\n\n\n\n\nThis will mark the task_1 as completed and schedule \ntask_5\n as the next task.\n\nRepeat the same process for the subsequently scheduled tasks until the completion.\n\n\n\n\nUsing Client Libraries\n\n\nConductor provides client libraries in Java (a Python client is works) to simplify task polling and execution.", 
            "title": "Kitchensink Example"
        }, 
        {
            "location": "/metadata/kitchensink/#definition", 
            "text": "{\n   name :  kitchensink ,\n   description :  kitchensink workflow ,\n   version : 1,\n   tasks : [\n    {\n       name :  task_1 ,\n       taskReferenceName :  task_1 ,\n       inputParameters : {\n         mod :  ${workflow.input.mod} ,\n         oddEven :  ${workflow.input.oddEven} \n      },\n       type :  SIMPLE \n    },\n    {\n       name :  event_task ,\n       taskReferenceName :  event_0 ,\n       inputParameters : {\n         mod :  ${workflow.input.mod} ,\n         oddEven :  ${workflow.input.oddEven} \n      },\n       type :  EVENT ,\n       sink :  conductor \n    },\n    {\n       name :  dyntask ,\n       taskReferenceName :  task_2 ,\n       inputParameters : {\n         taskToExecute :  ${workflow.input.task2Name} \n      },\n       type :  DYNAMIC ,\n       dynamicTaskNameParam :  taskToExecute \n    },\n    {\n       name :  oddEvenDecision ,\n       taskReferenceName :  oddEvenDecision ,\n       inputParameters : {\n         oddEven :  ${task_2.output.oddEven} \n      },\n       type :  DECISION ,\n       caseValueParam :  oddEven ,\n       decisionCases : {\n         0 : [\n          {\n             name :  task_4 ,\n             taskReferenceName :  task_4 ,\n             inputParameters : {\n               mod :  ${task_2.output.mod} ,\n               oddEven :  ${task_2.output.oddEven} \n            },\n             type :  SIMPLE \n          },\n          {\n             name :  dynamic_fanout ,\n             taskReferenceName :  fanout1 ,\n             inputParameters : {\n               dynamicTasks :  ${task_4.output.dynamicTasks} ,\n               input :  ${task_4.output.inputs} \n            },\n             type :  FORK_JOIN_DYNAMIC ,\n             dynamicForkTasksParam :  dynamicTasks ,\n             dynamicForkTasksInputParamName :  input \n          },\n          {\n             name :  dynamic_join ,\n             taskReferenceName :  join1 ,\n             type :  JOIN \n          }\n        ],\n         1 : [\n          {\n             name :  fork_join ,\n             taskReferenceName :  forkx ,\n             type :  FORK_JOIN ,\n             forkTasks : [\n              [\n                {\n                   name :  task_10 ,\n                   taskReferenceName :  task_10 ,\n                   type :  SIMPLE \n                },\n                {\n                   name :  sub_workflow_x ,\n                   taskReferenceName :  wf3 ,\n                   inputParameters : {\n                     mod :  ${task_1.output.mod} ,\n                     oddEven :  ${task_1.output.oddEven} \n                  },\n                   type :  SUB_WORKFLOW ,\n                   subWorkflowParam : {\n                     name :  sub_flow_1 ,\n                     version : 1\n                  }\n                }\n              ],\n              [\n                {\n                   name :  task_11 ,\n                   taskReferenceName :  task_11 ,\n                   type :  SIMPLE \n                },\n                {\n                   name :  sub_workflow_x ,\n                   taskReferenceName :  wf4 ,\n                   inputParameters : {\n                     mod :  ${task_1.output.mod} ,\n                     oddEven :  ${task_1.output.oddEven} \n                  },\n                   type :  SUB_WORKFLOW ,\n                   subWorkflowParam : {\n                     name :  sub_flow_1 ,\n                     version : 1\n                  }\n                }\n              ]\n            ]\n          },\n          {\n             name :  join ,\n             taskReferenceName :  join2 ,\n             type :  JOIN ,\n             joinOn : [\n               wf3 ,\n               wf4 \n            ]\n          }\n        ]\n      }\n    },\n    {\n       name :  search_elasticsearch ,\n       taskReferenceName :  get_es_1 ,\n       inputParameters : {\n         http_request : {\n           uri :  http://localhost:9200/conductor/_search?size=10 ,\n           method :  GET \n        }\n      },\n       type :  HTTP \n    },\n    {\n       name :  task_30 ,\n       taskReferenceName :  task_30 ,\n       inputParameters : {\n         statuses :  ${get_es_1.output..status} ,\n         workflowIds :  ${get_es_1.output..workflowId} \n      },\n       type :  SIMPLE \n    }\n  ],\n   outputParameters : {\n     statues :  ${get_es_1.output..status} ,\n     workflowIds :  ${get_es_1.output..workflowId} \n  },\n   schemaVersion : 2\n}", 
            "title": "Definition"
        }, 
        {
            "location": "/metadata/kitchensink/#visual-flow", 
            "text": "", 
            "title": "Visual Flow"
        }, 
        {
            "location": "/metadata/kitchensink/#running-kitchensink-workflow", 
            "text": "Start the server as documented  here .  Use  -DloadSample=true  java system property when launching the server.  This will create a kitchensink workflow, related task definition and kick off an instance of kitchensink workflow.  Once the workflow has started, the first task remains in the  SCHEDULED  state.  This is because no workers are currently polling for the task.  We will use the REST endpoints directly to poll for tasks and updating the status.", 
            "title": "Running Kitchensink Workflow"
        }, 
        {
            "location": "/metadata/kitchensink/#start-workflow-execution", 
            "text": "Start the execution of the kitchensink workflow by posting the following:  curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d '\n{\n     task2Name :  task_5  \n}\n'  The response is a text string identifying the workflow instance id.", 
            "title": "Start workflow execution"
        }, 
        {
            "location": "/metadata/kitchensink/#poll-for-the-first-task", 
            "text": "curl http://localhost:8080/api/tasks/poll/task_1  The response should look something like:  {\n     taskType :  task_1 ,\n     status :  IN_PROGRESS ,\n     inputData : {\n         mod : null,\n         oddEven : null\n    },\n     referenceTaskName :  task_1 ,\n     retryCount : 0,\n     seq : 1,\n     pollCount : 1,\n     taskDefName :  task_1 ,\n     scheduledTime : 1486580932471,\n     startTime : 1486580933869,\n     endTime : 0,\n     updateTime : 1486580933902,\n     startDelayInSeconds : 0,\n     retried : false,\n     callbackFromWorker : true,\n     responseTimeoutSeconds : 3600,\n     workflowInstanceId :  b0d1a935-3d74-46fd-92b2-0ca1e388659f ,\n     taskId :  b9eea7dd-3fbd-46b9-a9ff-b00279459476 ,\n     callbackAfterSeconds : 0,\n     polledTime : 1486580933902,\n     queueWaitTime : 1398\n}", 
            "title": "Poll for the first task:"
        }, 
        {
            "location": "/metadata/kitchensink/#update-the-task-status", 
            "text": "Note the values for  taskId  and  workflowInstanceId  fields from the poll response  Update the status of the task as  COMPLETED  as below:   curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d '\n{\n     taskId :  b9eea7dd-3fbd-46b9-a9ff-b00279459476 ,\n     workflowInstanceId :  b0d1a935-3d74-46fd-92b2-0ca1e388659f ,\n     status :  COMPLETED ,\n     output : {\n         mod : 5,\n         taskToExecute :  task_1 ,\n         oddEven : 0,\n         dynamicTasks : [\n            {\n                 name :  task_1 ,\n                 taskReferenceName :  task_1_1 ,\n                 type :  SIMPLE \n            },\n            {\n                 name :  sub_workflow_4 ,\n                 taskReferenceName :  wf_dyn ,\n                 type :  SUB_WORKFLOW ,\n                 subWorkflowParam : {\n                     name :  sub_flow_1 \n                }\n            }\n        ],\n         inputs : {\n             task_1_1 : {},\n             wf_dyn : {}\n        }\n    }\n}'  This will mark the task_1 as completed and schedule  task_5  as the next task. \nRepeat the same process for the subsequently scheduled tasks until the completion.   Using Client Libraries  Conductor provides client libraries in Java (a Python client is works) to simplify task polling and execution.", 
            "title": "Update the task status"
        }, 
        {
            "location": "/server/", 
            "text": "Installing\n\n\nRequirements\n\n\n\n\nDatabase\n: \nDynomite\n\n\nIndexing Backend\n: \nElasticsearch 2.x\n\n\nServlet Container\n: Tomcat, Jetty, or similar running JDK 1.8 or higher\n\n\n\n\nThere are 3 ways in which you can install Conductor:\n\n\n1. Build from source\n\n\nTo build from source, checkout the code from github and build server module using \ngradle build\n command. If you do not have gradle installed, you can run the command \n./gradlew build\n from the project root. This produces \nconductor-server-all-VERSION.jar\n in the folder \n./server/build/libs/\n\n\nThe jar can be executed using:\n\n\njava -jar conductor-server-VERSION-all.jar\n\n\n\n\n2. Download pre-built binaries from jcenter or maven central\n\n\nUse the following coordinates:\n\n\n\n\n\n\n\n\ngroup\n\n\nartifact\n\n\nversion\n\n\n\n\n\n\n\n\n\n\ncom.netflix.conductor\n\n\nconductor-server-all\n\n\n1.6.+\n\n\n\n\n\n\n\n\n3. Use the pre-configured Docker image\n\n\nTo build the docker images for the conductor server and ui run the commands:\n\n\ncd docker\ndocker-compose build\n\n\n\n\nAfter the docker images are built, run the following command to start the containers:\n\n\ndocker-compose up\n\n\n\n\nThis will create a docker container network that consists of the following images: conductor:server, conductor:ui, \nelasticsearch:2.4\n, and dynomite.\n\n\nTo view the UI, navigate to \nlocalhost:5000\n, to view the Swagger docs, navigate to \nlocalhost:8080\n.\n\n\nConfiguration\n\n\nConductor server uses a property file based configuration.  The property file is passed to the Main class as a command line argument.\n\n\njava -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path]\n\n\n\n\nlog4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).\n\n\nConfiguration Parameters\n\n\n\n# Database persistence model.  Possible values are memory, redis, and dynomite.\n# If omitted, the persistence used is memory\n#\n# memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo\n# redis : non-Dynomite based redis instance\n# dynomite : Dynomite cluster.  Use this for HA configuration.\ndb=dynomite\n\n# Dynomite Cluster details.\n# format is host:port:rack separated by semicolon\nworkflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e\n\n# Dynomite cluster name\nworkflow.dynomite.cluster.name=dyno_cluster_name\n\n# Namespace for the keys stored in Dynomite/Redis\nworkflow.namespace.prefix=conductor\n\n# Namespace prefix for the dyno queues\nworkflow.namespace.queue.prefix=conductor_queues\n\n# No. of threads allocated to dyno-queues (optional)\nqueues.dynomite.threads=10\n\n# Non-quorum port used to connect to local redis.  Used by dyno-queues.\n# When using redis directly, set this to the same port as redis server\n# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.\nqueues.dynomite.nonQuorum.port=22122\n\n# Transport address to elasticsearch\nworkflow.elasticsearch.url=localhost:9300\n\n# Name of the elasticsearch cluster\nworkflow.elasticsearch.index.name=conductor\n\n# Additional modules (optional)\nconductor.additional.modules=class_extending_com.google.inject.AbstractModule\n\n\n\n\n\nHigh Availability Configuration\n\n\nConductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs.  The scalability of the server is achieved by scaling the \nDynomite\n cluster along with \ndyno-queues\n which is used for queues.\n\n\nClients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).", 
            "title": "Conductor Server"
        }, 
        {
            "location": "/server/#installing", 
            "text": "", 
            "title": "Installing"
        }, 
        {
            "location": "/server/#requirements", 
            "text": "Database :  Dynomite  Indexing Backend :  Elasticsearch 2.x  Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher   There are 3 ways in which you can install Conductor:", 
            "title": "Requirements"
        }, 
        {
            "location": "/server/#1-build-from-source", 
            "text": "To build from source, checkout the code from github and build server module using  gradle build  command. If you do not have gradle installed, you can run the command  ./gradlew build  from the project root. This produces  conductor-server-all-VERSION.jar  in the folder  ./server/build/libs/  The jar can be executed using:  java -jar conductor-server-VERSION-all.jar", 
            "title": "1. Build from source"
        }, 
        {
            "location": "/server/#2-download-pre-built-binaries-from-jcenter-or-maven-central", 
            "text": "Use the following coordinates:     group  artifact  version      com.netflix.conductor  conductor-server-all  1.6.+", 
            "title": "2. Download pre-built binaries from jcenter or maven central"
        }, 
        {
            "location": "/server/#3-use-the-pre-configured-docker-image", 
            "text": "To build the docker images for the conductor server and ui run the commands:  cd docker\ndocker-compose build  After the docker images are built, run the following command to start the containers:  docker-compose up  This will create a docker container network that consists of the following images: conductor:server, conductor:ui,  elasticsearch:2.4 , and dynomite.  To view the UI, navigate to  localhost:5000 , to view the Swagger docs, navigate to  localhost:8080 .", 
            "title": "3. Use the pre-configured Docker image"
        }, 
        {
            "location": "/server/#configuration", 
            "text": "Conductor server uses a property file based configuration.  The property file is passed to the Main class as a command line argument.  java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path]  log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).", 
            "title": "Configuration"
        }, 
        {
            "location": "/server/#configuration-parameters", 
            "text": "# Database persistence model.  Possible values are memory, redis, and dynomite.\n# If omitted, the persistence used is memory\n#\n# memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo\n# redis : non-Dynomite based redis instance\n# dynomite : Dynomite cluster.  Use this for HA configuration.\ndb=dynomite\n\n# Dynomite Cluster details.\n# format is host:port:rack separated by semicolon\nworkflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e\n\n# Dynomite cluster name\nworkflow.dynomite.cluster.name=dyno_cluster_name\n\n# Namespace for the keys stored in Dynomite/Redis\nworkflow.namespace.prefix=conductor\n\n# Namespace prefix for the dyno queues\nworkflow.namespace.queue.prefix=conductor_queues\n\n# No. of threads allocated to dyno-queues (optional)\nqueues.dynomite.threads=10\n\n# Non-quorum port used to connect to local redis.  Used by dyno-queues.\n# When using redis directly, set this to the same port as redis server\n# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.\nqueues.dynomite.nonQuorum.port=22122\n\n# Transport address to elasticsearch\nworkflow.elasticsearch.url=localhost:9300\n\n# Name of the elasticsearch cluster\nworkflow.elasticsearch.index.name=conductor\n\n# Additional modules (optional)\nconductor.additional.modules=class_extending_com.google.inject.AbstractModule", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/server/#high-availability-configuration", 
            "text": "Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs.  The scalability of the server is achieved by scaling the  Dynomite  cluster along with  dyno-queues  which is used for queues.  Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).", 
            "title": "High Availability Configuration"
        }, 
        {
            "location": "/worker/", 
            "text": "Conductor tasks executed by remote workers communicates over HTTP endpoints to poll for the task and updates the status of the execution.\n\n\nConductor provides a framework to poll for tasks, manage the execution thread and update the status of the execution back to the server.  The framework provides libraries in Java and Python.  Other language support can be added by using the HTTP endpoints for task management.\n\n\nJava\n\n\n\n\nImplement \nWorker\n interface to implement the task.\n\n\n\n\nUse \nWorkflowTaskCoordinator\n to register the worker(s) and initialize the polling loop. \n\n\n\n\nSample Worker Implementation\n\n\nExample\n\n\n\n\n\n\n\n\nWorkflowTaskCoordinator\n\n\nManages the Task workers thread pool and server communication (poll, task update and ack).\n\n\nWorker\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npaused\n\n\nboolean.  If set to true, the worker stops polling.\n\n\n\n\n\n\npollCount\n\n\nNo. of tasks to poll for.  Used for batched polling.  Each task is executed in a separate thread.\n\n\n\n\n\n\nlongPollTimeout\n\n\nTime in millisecond for long polling to Conductor server for tasks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese properties can be set either by Worker implementation or by setting the following system properties in the JVM:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconductor.worker.\nproperty\n\n\nApplies to ALL the workers in the JVM\n\n\n\n\n\n\nconductor.worker.\ntaskDefName\n.\nproperty\n\n\nApplies to the specified worker.  Overrides the global property.\n\n\n\n\n\n\n\n\nPython\n\n\nhttps://github.com/Netflix/conductor/tree/dev/client/python\n\n\nFollow the example as documented in the readme or take a look at \nkitchensink_workers.py\n\n\n\n\nWarning\n\n\nPython client is under development is not production battle tested.  We encourage you to test it out and let us know the feedback.  Pull Requests with fixes or enhancements are welcomed!", 
            "title": "Conductor Task Workers"
        }, 
        {
            "location": "/worker/#java", 
            "text": "Implement  Worker  interface to implement the task.   Use  WorkflowTaskCoordinator  to register the worker(s) and initialize the polling loop.    Sample Worker Implementation  Example", 
            "title": "Java"
        }, 
        {
            "location": "/worker/#workflowtaskcoordinator", 
            "text": "Manages the Task workers thread pool and server communication (poll, task update and ack).", 
            "title": "WorkflowTaskCoordinator"
        }, 
        {
            "location": "/worker/#worker", 
            "text": "Property  Description      paused  boolean.  If set to true, the worker stops polling.    pollCount  No. of tasks to poll for.  Used for batched polling.  Each task is executed in a separate thread.    longPollTimeout  Time in millisecond for long polling to Conductor server for tasks         These properties can be set either by Worker implementation or by setting the following system properties in the JVM:           conductor.worker. property  Applies to ALL the workers in the JVM    conductor.worker. taskDefName . property  Applies to the specified worker.  Overrides the global property.", 
            "title": "Worker"
        }, 
        {
            "location": "/worker/#python", 
            "text": "https://github.com/Netflix/conductor/tree/dev/client/python  Follow the example as documented in the readme or take a look at  kitchensink_workers.py   Warning  Python client is under development is not production battle tested.  We encourage you to test it out and let us know the feedback.  Pull Requests with fixes or enhancements are welcomed!", 
            "title": "Python"
        }, 
        {
            "location": "/domains/", 
            "text": "Task Domains\n\n\nTask domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it.  \n\n\nAs an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task.\n\n\nWhen starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active then the task is schedule with no domain (default behavior). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. \n\n\nHow to use Task Domains\n\n\nChange the poll call\n\n\nThe poll call must now specify the domain. \n\n\nJava Client\n\n\nIf you are using the java client then a simple property change will force  WorkflowTaskCoordinator to pass the domain to the poller.\n\n\n    conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \nmydomain\n\n\n\n\n\nREST call\n\n\nGET /tasks/poll/batch/T2?workerid=myworker\ndomain=mydomain\n\n\nGET /tasks/poll/T2?workerid=myworker\ndomain=mydomain\n\n\nChange the start workflow call\n\n\nWhen starting the workflow, make sure the task to domain mapping is passes\n\n\nJava Client\n\n\n    Map\nString, Object\n input = new HashMap\n();\n    input.put(\nwf_input1\n, \none\u201d);\n\n    Map\nString, String\n taskToDomain = new HashMap\n();\n    taskToDomain.put(\nT2\n, \nmydomain\n);\n\n    // Other options ...\n    // taskToDomain.put(\n*\n, \nmydomain\n) will put all tasks in mydomain\n    // taskToDomain.put(\nT2\n, \nmydomain,fallbackDomain\n) If mydomain has no active workers\n    //        for T2 then will be put in fallbackDomain. Same can be used with \n*\n too.\n\n    StartWorkflowRequest swr = new StartWorkflowRequest();\n    swr.withName(\u201cmyWorkflow\u201d)\n        .withCorrelationId(\u201ccorr1\u201d)\n        .withVersion(1)\n        .withInput(input)\n        .withTaskToDomain(taskToDomain);\n\n    wfclient.startWorkflow(swr);\n\n\n\n\n\nREST call\n\n\nPOST /workflow\n\n\n{\n  \nname\n: \nmyWorkflow\n,\n  \nversion\n: 1,\n  \ncorrelatonId\n: \ncorr1\n\n  \ninput\n: {\n    \nwf_input1\n: \none\n\n  },\n  \ntaskToDomain\n: {\n    \nT2\n: \nmydomain\n\n  }\n}", 
            "title": "Task Domains"
        }, 
        {
            "location": "/domains/#task-domains", 
            "text": "Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it.    As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task.  When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active then the task is schedule with no domain (default behavior). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used.", 
            "title": "Task Domains"
        }, 
        {
            "location": "/domains/#how-to-use-task-domains", 
            "text": "", 
            "title": "How to use Task Domains"
        }, 
        {
            "location": "/domains/#change-the-poll-call", 
            "text": "The poll call must now specify the domain.", 
            "title": "Change the poll call"
        }, 
        {
            "location": "/domains/#java-client", 
            "text": "If you are using the java client then a simple property change will force  WorkflowTaskCoordinator to pass the domain to the poller.      conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain  mydomain", 
            "title": "Java Client"
        }, 
        {
            "location": "/domains/#rest-call", 
            "text": "GET /tasks/poll/batch/T2?workerid=myworker domain=mydomain  GET /tasks/poll/T2?workerid=myworker domain=mydomain", 
            "title": "REST call"
        }, 
        {
            "location": "/domains/#change-the-start-workflow-call", 
            "text": "When starting the workflow, make sure the task to domain mapping is passes", 
            "title": "Change the start workflow call"
        }, 
        {
            "location": "/domains/#java-client_1", 
            "text": "Map String, Object  input = new HashMap ();\n    input.put( wf_input1 ,  one\u201d);\n\n    Map String, String  taskToDomain = new HashMap ();\n    taskToDomain.put( T2 ,  mydomain );\n\n    // Other options ...\n    // taskToDomain.put( * ,  mydomain ) will put all tasks in mydomain\n    // taskToDomain.put( T2 ,  mydomain,fallbackDomain ) If mydomain has no active workers\n    //        for T2 then will be put in fallbackDomain. Same can be used with  *  too.\n\n    StartWorkflowRequest swr = new StartWorkflowRequest();\n    swr.withName(\u201cmyWorkflow\u201d)\n        .withCorrelationId(\u201ccorr1\u201d)\n        .withVersion(1)\n        .withInput(input)\n        .withTaskToDomain(taskToDomain);\n\n    wfclient.startWorkflow(swr);", 
            "title": "Java Client"
        }, 
        {
            "location": "/domains/#rest-call_1", 
            "text": "POST /workflow  {\n   name :  myWorkflow ,\n   version : 1,\n   correlatonId :  corr1 \n   input : {\n     wf_input1 :  one \n  },\n   taskToDomain : {\n     T2 :  mydomain \n  }\n}", 
            "title": "REST call"
        }, 
        {
            "location": "/extend/", 
            "text": "Backend\n\n\nConductor provides a pluggable backend.  The current implementation uses Dynomite.\n\n\nThere are 4 interfaces that needs to be implemented for each backend:\n\n\n//Store for workflow and task definitions\ncom.netflix.conductor.dao.MetadataDAO\n\n\n\n\n//Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO\n\n\n\n\n//Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO\n\n\n\n\n//Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO\n\n\n\n\nIt is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.\n\n\nSystem Tasks\n\n\nTo create system tasks follow the steps below:\n\n\n\n\nExtend \ncom.netflix.conductor.core.execution.tasks.WorkflowSystemTask\n\n\nInstantiate the new class as part of the startup (eager singleton)", 
            "title": "Extending Conductor"
        }, 
        {
            "location": "/extend/#backend", 
            "text": "Conductor provides a pluggable backend.  The current implementation uses Dynomite.  There are 4 interfaces that needs to be implemented for each backend:  //Store for workflow and task definitions\ncom.netflix.conductor.dao.MetadataDAO  //Store for workflow executions\ncom.netflix.conductor.dao.ExecutionDAO  //Index for workflow executions\ncom.netflix.conductor.dao.IndexDAO  //Queue provider for tasks\ncom.netflix.conductor.dao.QueueDAO  It is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.", 
            "title": "Backend"
        }, 
        {
            "location": "/extend/#system-tasks", 
            "text": "To create system tasks follow the steps below:   Extend  com.netflix.conductor.core.execution.tasks.WorkflowSystemTask  Instantiate the new class as part of the startup (eager singleton)", 
            "title": "System Tasks"
        }, 
        {
            "location": "/runtime/", 
            "text": "Task \n Workflow Metadata\n\n\n\n\n\n\n\n\nEndpoint\n\n\nDescription\n\n\nInput\n\n\n\n\n\n\n\n\n\n\nGET /metadata/taskdefs\n\n\nGet all the task definitions\n\n\nn/a\n\n\n\n\n\n\nGET /metadata/taskdefs/{taskType}\n\n\nRetrieve task definition\n\n\nTask Name\n\n\n\n\n\n\nPOST /metadata/taskdefs\n\n\nRegister new task definitions\n\n\nList of \nTask Definitions\n\n\n\n\n\n\nPUT /metadata/taskdefs\n\n\nUpdate a task definition\n\n\nA \nTask Definition\n\n\n\n\n\n\nDELETE /metadata/taskdefs/{taskType}\n\n\nDelete a task definition\n\n\nTask Name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGET /metadata/workflow\n\n\nGet all the workflow definitions\n\n\nn/a\n\n\n\n\n\n\nPOST /metadata/workflow\n\n\nRegister new workflow\n\n\nWorkflow Definition\n\n\n\n\n\n\nPUT /metadata/workflow\n\n\nRegister/Update new workflows\n\n\nList of \nWorkflow Definition\n\n\n\n\n\n\nGET /metadata/workflow/{name}?version=\n\n\nGet the workflow definitions\n\n\nworkflow name, version (optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart A Workflow\n\n\nWith Input only\n\n\nPOST /workflow/{name}?version=\ncorrelationId=\n{\n   //JSON payload for workflow\n}\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nversion\n\n\nOptional.  If not specified uses the latest version of the workflow\n\n\n\n\n\n\ncorrelationId\n\n\nUser supplied Id that can be used to retrieve workflows\n\n\n\n\n\n\n\n\nInput\n\n\nJSON Payload to start the workflow.  Mandatory.  If workflow does not expect any input MUST pass an empty JSON like \n{}\n\n\nOutput\n\n\nId of the workflow (GUID)\n\n\nWith Input and Task Domains\n\n\nPOST /workflow\n{\n   //JSON payload for Start workflow request\n}\n\n\n\n\nStart workflow request\n\n\nJSON for start workflow request\n\n\n{\n  \nname\n: \nmyWorkflow\n, // Name of the workflow\n  \nversion\n: 1, // Version\n  \u201ccorrelatond\u201d: \u201ccorr1\u201d // correlation Id\n  \ninput\n: {\n    // Input map. \n  },\n  \ntaskToDomain\n: {\n    // Task to domain map\n  }\n}\n\n\n\n\nOutput\n\n\nId of the workflow (GUID)\n\n\nRetrieve Workflows\n\n\n\n\n\n\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET /workflow/{workflowId}?includeTasks=true|false\n\n\nGet Workflow State by workflow Id.  If includeTasks is set, then also includes all the tasks executed and scheduled.\n\n\n\n\n\n\nGET /workflow/running/{name}\n\n\nGet all the running workflows of a given type\n\n\n\n\n\n\nGET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false\nincludeTasks=true|false\n\n\nGet all the running workflows filtered by correlation Id.  If includeClosed is set, also includes workflows that have completed running.\n\n\n\n\n\n\nGET /workflow/search\n\n\nSearch for workflows.  See Below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch for Workflows\n\n\nConductor uses Elasticsearch for indexing workflow execution and is used by search APIs.\n\n\nGET /workflow/search?start=\nsize=\nsort=\nfreeText=\nquery=\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstart\n\n\nPage number.  Defaults to 0\n\n\n\n\n\n\nsize\n\n\nNumber of results to return\n\n\n\n\n\n\nsort\n\n\nSorting.  Format is: \nASC:\nfieldname\n or \nDESC:\nfieldname\n to sort in ascending or descending order by a field\n\n\n\n\n\n\nfreeText\n\n\nElasticsearch supported query. e.g. workflowType:\"name_of_workflow\"\n\n\n\n\n\n\nquery\n\n\nSQL like where clause.  e.g. workflowType = 'name_of_workflow'.  Optional if freeText is provided.\n\n\n\n\n\n\n\n\nOutput\n\n\nSearch result as described below:\n\n\n{\n  \ntotalHits\n: 0,\n  \nresults\n: [\n    {\n      \nworkflowType\n: \nstring\n,\n      \nversion\n: 0,\n      \nworkflowId\n: \nstring\n,\n      \ncorrelationId\n: \nstring\n,\n      \nstartTime\n: \nstring\n,\n      \nupdateTime\n: \nstring\n,\n      \nendTime\n: \nstring\n,\n      \nstatus\n: \nRUNNING\n,\n      \ninput\n: \nstring\n,\n      \noutput\n: \nstring\n,\n      \nreasonForIncompletion\n: \nstring\n,\n      \nexecutionTime\n: 0,\n      \nevent\n: \nstring\n\n    }\n  ]\n}\n\n\n\n\nManage Workflows\n\n\n\n\n\n\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPUT /workflow/{workflowId}/pause\n\n\nPause.  No further tasks will be scheduled until resumed.  Currently running tasks are not paused.\n\n\n\n\n\n\nPUT /workflow/{workflowId}/resume\n\n\nResume normal operations after a pause.\n\n\n\n\n\n\nPOST /workflow/{workflowId}/rerun\n\n\nSee Below.\n\n\n\n\n\n\nPOST /workflow/{workflowId}/restart\n\n\nRestart workflow execution from the start.  Current execution history is wiped out.\n\n\n\n\n\n\nPOST /workflow/{workflowId}/retry\n\n\nRetry the last failed task.\n\n\n\n\n\n\nPUT /workflow/{workflowId}/skiptask/{taskReferenceName}\n\n\nSee below.\n\n\n\n\n\n\nDELETE /workflow/{workflowId}\n\n\nTerminates the running workflow.\n\n\n\n\n\n\nDELETE /workflow/{workflowId}/remove\n\n\nDeletes the workflow from system.  Use with caution.\n\n\n\n\n\n\n\n\nRerun\n\n\nRe-runs a completed workflow from a specific task. \n\n\nPOST /workflow/{workflowId}/rerun\n\n\n{\n  \nreRunFromWorkflowId\n: \nstring\n,\n  \nworkflowInput\n: {},\n  \nreRunFromTaskId\n: \nstring\n,\n  \ntaskInput\n: {}\n}\n\n\n\n\nSkip Task\n\n\nSkips a task execution (specified as \ntaskReferenceName\n parameter) in a running workflow and continues forward.\nOptionally updating task's input and output as specified in the payload.\n\nPUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=\ntaskReferenceName=\n\n\n{\n  \ntaskInput\n: {},\n  \ntaskOutput\n: {}\n}\n\n\n\n\nManage Tasks\n\n\n\n\n\n\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET /tasks/{taskId}\n\n\nGet task details.\n\n\n\n\n\n\nGET /tasks/queue/all\n\n\nList the pending task sizes.\n\n\n\n\n\n\nGET /tasks/queue/all/verbose\n\n\nSame as above, includes the size per shard\n\n\n\n\n\n\nGET /tasks/queue/sizes?taskType=\ntaskType=\ntaskType\n\n\nReturn the size of pending tasks for given task types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolling, Ack and Update Task\n\n\nThese are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker.\n\n\n\n\n\n\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET /tasks/poll/{taskType}?workerid=\ndomain=\n\n\nPoll for a task. \nworkerid\n identifies the worker that polled for the job and \ndomain\n allows the poller to poll for a task in a specific domain\n\n\n\n\n\n\nGET /tasks/poll/batch/{taskType}?count=\ntimeout=\nworkerid=\ndomain\n\n\nPoll for a task in a batch specified by \ncount\n.  This is a long poll and the connection will wait until \ntimeout\n or if there is at-least 1 item available, whichever comes first.\nworkerid\n identifies the worker that polled for the job and \ndomain\n allows the poller to poll for a task in a specific domain\n\n\n\n\n\n\nPOST /tasks\n\n\nUpdate the result of task execution.  See the schema below.\n\n\n\n\n\n\nPOST /tasks/{taskId}/ack\n\n\nAcknowledges the task received AFTER poll by worker.\n\n\n\n\n\n\n\n\nSchema for updating Task Result\n\n\n{\n    \nworkflowInstanceId\n: \nWorkflow Instance Id\n,\n    \ntaskId\n: \nID of the task to be updated\n,\n    \nreasonForIncompletion\n : \nIf failed, reason for failure\n,\n    \ncallbackAfterSeconds\n: 0,\n    \nstatus\n: \nIN_PROGRESS|FAILED|COMPLETED\n,\n    \noutputData\n: {\n        //JSON document representing Task execution output     \n    }\n\n}\n\n\n\n\n\n\nAcknowledging tasks after poll\n\n\nIf the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.", 
            "title": "APIs"
        }, 
        {
            "location": "/runtime/#task-workflow-metadata", 
            "text": "Endpoint  Description  Input      GET /metadata/taskdefs  Get all the task definitions  n/a    GET /metadata/taskdefs/{taskType}  Retrieve task definition  Task Name    POST /metadata/taskdefs  Register new task definitions  List of  Task Definitions    PUT /metadata/taskdefs  Update a task definition  A  Task Definition    DELETE /metadata/taskdefs/{taskType}  Delete a task definition  Task Name         GET /metadata/workflow  Get all the workflow definitions  n/a    POST /metadata/workflow  Register new workflow  Workflow Definition    PUT /metadata/workflow  Register/Update new workflows  List of  Workflow Definition    GET /metadata/workflow/{name}?version=  Get the workflow definitions  workflow name, version (optional)", 
            "title": "Task &amp; Workflow Metadata"
        }, 
        {
            "location": "/runtime/#start-a-workflow", 
            "text": "", 
            "title": "Start A Workflow"
        }, 
        {
            "location": "/runtime/#with-input-only", 
            "text": "POST /workflow/{name}?version= correlationId=\n{\n   //JSON payload for workflow\n}     Parameter  Description      version  Optional.  If not specified uses the latest version of the workflow    correlationId  User supplied Id that can be used to retrieve workflows", 
            "title": "With Input only"
        }, 
        {
            "location": "/runtime/#input", 
            "text": "JSON Payload to start the workflow.  Mandatory.  If workflow does not expect any input MUST pass an empty JSON like  {}", 
            "title": "Input"
        }, 
        {
            "location": "/runtime/#output", 
            "text": "Id of the workflow (GUID)", 
            "title": "Output"
        }, 
        {
            "location": "/runtime/#with-input-and-task-domains", 
            "text": "POST /workflow\n{\n   //JSON payload for Start workflow request\n}", 
            "title": "With Input and Task Domains"
        }, 
        {
            "location": "/runtime/#start-workflow-request", 
            "text": "JSON for start workflow request  {\n   name :  myWorkflow , // Name of the workflow\n   version : 1, // Version\n  \u201ccorrelatond\u201d: \u201ccorr1\u201d // correlation Id\n   input : {\n    // Input map. \n  },\n   taskToDomain : {\n    // Task to domain map\n  }\n}", 
            "title": "Start workflow request"
        }, 
        {
            "location": "/runtime/#output_1", 
            "text": "Id of the workflow (GUID)", 
            "title": "Output"
        }, 
        {
            "location": "/runtime/#retrieve-workflows", 
            "text": "Endpoint  Description      GET /workflow/{workflowId}?includeTasks=true|false  Get Workflow State by workflow Id.  If includeTasks is set, then also includes all the tasks executed and scheduled.    GET /workflow/running/{name}  Get all the running workflows of a given type    GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false includeTasks=true|false  Get all the running workflows filtered by correlation Id.  If includeClosed is set, also includes workflows that have completed running.    GET /workflow/search  Search for workflows.  See Below.", 
            "title": "Retrieve Workflows"
        }, 
        {
            "location": "/runtime/#search-for-workflows", 
            "text": "Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs.  GET /workflow/search?start= size= sort= freeText= query=     Parameter  Description      start  Page number.  Defaults to 0    size  Number of results to return    sort  Sorting.  Format is:  ASC: fieldname  or  DESC: fieldname  to sort in ascending or descending order by a field    freeText  Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\"    query  SQL like where clause.  e.g. workflowType = 'name_of_workflow'.  Optional if freeText is provided.", 
            "title": "Search for Workflows"
        }, 
        {
            "location": "/runtime/#output_2", 
            "text": "Search result as described below:  {\n   totalHits : 0,\n   results : [\n    {\n       workflowType :  string ,\n       version : 0,\n       workflowId :  string ,\n       correlationId :  string ,\n       startTime :  string ,\n       updateTime :  string ,\n       endTime :  string ,\n       status :  RUNNING ,\n       input :  string ,\n       output :  string ,\n       reasonForIncompletion :  string ,\n       executionTime : 0,\n       event :  string \n    }\n  ]\n}", 
            "title": "Output"
        }, 
        {
            "location": "/runtime/#manage-workflows", 
            "text": "Endpoint  Description      PUT /workflow/{workflowId}/pause  Pause.  No further tasks will be scheduled until resumed.  Currently running tasks are not paused.    PUT /workflow/{workflowId}/resume  Resume normal operations after a pause.    POST /workflow/{workflowId}/rerun  See Below.    POST /workflow/{workflowId}/restart  Restart workflow execution from the start.  Current execution history is wiped out.    POST /workflow/{workflowId}/retry  Retry the last failed task.    PUT /workflow/{workflowId}/skiptask/{taskReferenceName}  See below.    DELETE /workflow/{workflowId}  Terminates the running workflow.    DELETE /workflow/{workflowId}/remove  Deletes the workflow from system.  Use with caution.", 
            "title": "Manage Workflows"
        }, 
        {
            "location": "/runtime/#rerun", 
            "text": "Re-runs a completed workflow from a specific task.   POST /workflow/{workflowId}/rerun  {\n   reRunFromWorkflowId :  string ,\n   workflowInput : {},\n   reRunFromTaskId :  string ,\n   taskInput : {}\n}", 
            "title": "Rerun"
        }, 
        {
            "location": "/runtime/#skip-task", 
            "text": "Skips a task execution (specified as  taskReferenceName  parameter) in a running workflow and continues forward.\nOptionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId= taskReferenceName=  {\n   taskInput : {},\n   taskOutput : {}\n}", 
            "title": "Skip Task"
        }, 
        {
            "location": "/runtime/#manage-tasks", 
            "text": "Endpoint  Description      GET /tasks/{taskId}  Get task details.    GET /tasks/queue/all  List the pending task sizes.    GET /tasks/queue/all/verbose  Same as above, includes the size per shard    GET /tasks/queue/sizes?taskType= taskType= taskType  Return the size of pending tasks for given task types", 
            "title": "Manage Tasks"
        }, 
        {
            "location": "/runtime/#polling-ack-and-update-task", 
            "text": "These are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker.     Endpoint  Description      GET /tasks/poll/{taskType}?workerid= domain=  Poll for a task.  workerid  identifies the worker that polled for the job and  domain  allows the poller to poll for a task in a specific domain    GET /tasks/poll/batch/{taskType}?count= timeout= workerid= domain  Poll for a task in a batch specified by  count .  This is a long poll and the connection will wait until  timeout  or if there is at-least 1 item available, whichever comes first. workerid  identifies the worker that polled for the job and  domain  allows the poller to poll for a task in a specific domain    POST /tasks  Update the result of task execution.  See the schema below.    POST /tasks/{taskId}/ack  Acknowledges the task received AFTER poll by worker.", 
            "title": "Polling, Ack and Update Task"
        }, 
        {
            "location": "/runtime/#schema-for-updating-task-result", 
            "text": "{\n     workflowInstanceId :  Workflow Instance Id ,\n     taskId :  ID of the task to be updated ,\n     reasonForIncompletion  :  If failed, reason for failure ,\n     callbackAfterSeconds : 0,\n     status :  IN_PROGRESS|FAILED|COMPLETED ,\n     outputData : {\n        //JSON document representing Task execution output     \n    }\n\n}   Acknowledging tasks after poll  If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.", 
            "title": "Schema for updating Task Result"
        }, 
        {
            "location": "/metrics/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\nworkflow_server_error\n\n\nRate at which server side error is happening\n\n\nmethodName\n\n\n\n\n\n\nworkflow_failure\n\n\nCounter for failing workflows\n\n\nworkflowName, status\n\n\n\n\n\n\nworkflow_start_error\n\n\nCounter for failing to start a workflow\n\n\nworkflowName\n\n\n\n\n\n\nworkflow_running\n\n\nCounter for no. of running workflows\n\n\nworkflowName, version\n\n\n\n\n\n\ntask_queue_wait\n\n\nTime spent by a task in queue\n\n\ntaskType\n\n\n\n\n\n\ntask_execution\n\n\nTime taken to execute a task\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_poll\n\n\nTime taken to poll for a task\n\n\ntaskType\n\n\n\n\n\n\ntask_queue_depth\n\n\nPending tasks queue depth\n\n\ntaskType\n\n\n\n\n\n\ntask_timeout\n\n\nCounter for timed out tasks\n\n\ntaskType", 
            "title": "Server Metrics"
        }, 
        {
            "location": "/metrics/client/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\nWhen using the Java client, the following metrics are published:\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\ntask_execution_queue_full\n\n\nCounter to record execution queue has saturated\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_error\n\n\nClient error when polling for a task queue\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_execute_error\n\n\nExecution error\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_failed\n\n\nTask ack failed\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_error\n\n\nTask ack has encountered an exception\n\n\ntaskType\n\n\n\n\n\n\ntask_update_error\n\n\nTask status cannot be updated back to server\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_counter\n\n\nIncremented each time polling is done\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_time\n\n\nTime to poll for a batch of tasks\n\n\ntaskType\n\n\n\n\n\n\ntask_execute_time\n\n\nTime to execute a task\n\n\ntaskType\n\n\n\n\n\n\n\n\nMetrics on client side supplements the one collected from server in identifying the network as well as client side issues.", 
            "title": "Worker Metrics"
        }, 
        {
            "location": "/faq/", 
            "text": "How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)\n\n\nAfter polling for the task update the status of the task to \nIN_PROGRESS\n and set the \ncallbackAfterSeconds\n value to the desired time.  The task will remain in the queue until the specified second before worker polling for it will receive it again.\n\n\nIf there is a timeout set for the task, and the \ncallbackAfterSeconds\n exceeds the timeout value, it will result in task being TIMED_OUT.\n\n\nHow long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?\n\n\nYes.  As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.\n\n\nMy workflow fails to start with missing task error\n\n\nEnsure all the tasks are registered via \n/metadata/taskdefs\n APIs.  Add any missing task definition (as reported in the error) and try again.\n\n\nWhere does my worker run?  How does conductor run my tasks?\n\n\nConductor does not run the workers.  When a task is scheduled, it is put into the queue maintained by Conductor.  Workers are required to poll for tasks using \n/tasks/poll\n API at periodic interval, execute the business logic for the task and report back the results using \nPOST /tasks\n API call. \nConductor, however will run \nsystem tasks\n on the Conductor server.\n\n\nHow can I schedule workflows to run at a specific time?\n\n\nConductor does not provide any scheduling mechanism.  But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow.  Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow.\n\nMore details about \neventing\n.\n\n\nHow do I setup Dynomite cluster?\n\n\nVisit Dynomite's github page.  \nhttps://github.com/Netflix/dynomite\n to find details on setup and support mechanism.\n\n\nCan I use conductor with Ruby / Go / Python?\n\n\nYes.  Workers can be written any language as long as they can poll and update the task results via HTTP endpoints.\n\n\nConductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server.\n\n\nNote:\n Python client is currently in development and not battle tested for production use cases. \n\n\nHow can I get help with Dynomite?\n\n\nVisit Dynomite's github page.  \nhttps://github.com/Netflix/dynomite\n to find details on setup and support mechanism.", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc", 
            "text": "After polling for the task update the status of the task to  IN_PROGRESS  and set the  callbackAfterSeconds  value to the desired time.  The task will remain in the queue until the specified second before worker polling for it will receive it again.  If there is a timeout set for the task, and the  callbackAfterSeconds  exceeds the timeout value, it will result in task being TIMED_OUT.", 
            "title": "How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)"
        }, 
        {
            "location": "/faq/#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months", 
            "text": "Yes.  As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.", 
            "title": "How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?"
        }, 
        {
            "location": "/faq/#my-workflow-fails-to-start-with-missing-task-error", 
            "text": "Ensure all the tasks are registered via  /metadata/taskdefs  APIs.  Add any missing task definition (as reported in the error) and try again.", 
            "title": "My workflow fails to start with missing task error"
        }, 
        {
            "location": "/faq/#where-does-my-worker-run-how-does-conductor-run-my-tasks", 
            "text": "Conductor does not run the workers.  When a task is scheduled, it is put into the queue maintained by Conductor.  Workers are required to poll for tasks using  /tasks/poll  API at periodic interval, execute the business logic for the task and report back the results using  POST /tasks  API call. \nConductor, however will run  system tasks  on the Conductor server.", 
            "title": "Where does my worker run?  How does conductor run my tasks?"
        }, 
        {
            "location": "/faq/#how-can-i-schedule-workflows-to-run-at-a-specific-time", 
            "text": "Conductor does not provide any scheduling mechanism.  But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow.  Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. \nMore details about  eventing .", 
            "title": "How can I schedule workflows to run at a specific time?"
        }, 
        {
            "location": "/faq/#how-do-i-setup-dynomite-cluster", 
            "text": "Visit Dynomite's github page.   https://github.com/Netflix/dynomite  to find details on setup and support mechanism.", 
            "title": "How do I setup Dynomite cluster?"
        }, 
        {
            "location": "/faq/#can-i-use-conductor-with-ruby-go-python", 
            "text": "Yes.  Workers can be written any language as long as they can poll and update the task results via HTTP endpoints.  Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server.  Note:  Python client is currently in development and not battle tested for production use cases.", 
            "title": "Can I use conductor with Ruby / Go / Python?"
        }, 
        {
            "location": "/faq/#how-can-i-get-help-with-dynomite", 
            "text": "Visit Dynomite's github page.   https://github.com/Netflix/dynomite  to find details on setup and support mechanism.", 
            "title": "How can I get help with Dynomite?"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright 2016 Netflix, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "License"
        }
    ]
}